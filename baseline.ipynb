{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4792b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def read_data(path):\n",
    "    sentence_a, sentence_b, labels = [], [], []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f.readlines(), desc='Reading data'):\n",
    "            line = json.loads(line)\n",
    "            sentence_a.append(line['sentence1'])\n",
    "            sentence_b.append(line['sentence2'])\n",
    "            labels.append(int(line['label']))\n",
    "    df = pd.DataFrame(zip(sentence_a, sentence_b, labels), columns=['text_a', 'text_b', 'labels'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f757e284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3705f9b0c40f47ba8b42456cd0b6ba1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading data:   0%|          | 0/34334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = read_data('data/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "099c4ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>蚂蚁借呗等额还款可以换成先息后本吗</td>\n",
       "      <td>借呗有先息到期还本吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>蚂蚁花呗说我违约一次</td>\n",
       "      <td>蚂蚁花呗违约行为是什么</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>帮我看一下本月花呗账单有没有结清</td>\n",
       "      <td>下月花呗账单</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>蚂蚁借呗多长时间综合评估一次</td>\n",
       "      <td>借呗得评估多久</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>我的花呗账单是***，还款怎么是***</td>\n",
       "      <td>我的花呗，月结出来说让我还***元，我自己算了一下详细名单我应该还***元</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text_a                                 text_b  labels\n",
       "0    蚂蚁借呗等额还款可以换成先息后本吗                             借呗有先息到期还本吗       0\n",
       "1           蚂蚁花呗说我违约一次                            蚂蚁花呗违约行为是什么       0\n",
       "2     帮我看一下本月花呗账单有没有结清                                 下月花呗账单       0\n",
       "3       蚂蚁借呗多长时间综合评估一次                                借呗得评估多久       0\n",
       "4  我的花呗账单是***，还款怎么是***  我的花呗，月结出来说让我还***元，我自己算了一下详细名单我应该还***元       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "082e34dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b372bce68524fbf97bb23f2a3732a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading data:   0%|          | 0/4316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_df = read_data('data/dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9adf5e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(train_df.text_a.str.len() + train_df.text_b.str.len()).hist(bins=20)\n",
    "(dev_df.text_a.str.len() + dev_df.text_b.str.len()).hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "661f0a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_df.text_a.str.len() + train_df.text_b.str.len()).quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90d69762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "config = {\n",
    "    'train_file_path': 'data/train.json',\n",
    "    'dev_file_path': 'data/dev.json',\n",
    "    'test_file_path': 'data/test.json',\n",
    "    'embedding_file_path': '../NLP_Project/dataset/sgns.weibo.word.bz2',\n",
    "        'train_val_ratio': 0.1,\n",
    "        'vocab_size': 30000,\n",
    "        'batch_size': 64,\n",
    "        'max_seq_len':64,\n",
    "        'num_epochs': 1,\n",
    "        'learning_rate': 1e-3,\n",
    "        'logging_step': 200,\n",
    "        'seed': 2021\n",
    "}\n",
    "config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64b20fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import jieba\n",
    "import bz2\n",
    "\n",
    "def preprocess(config):\n",
    "    \n",
    "    def convert2df(file_path, dataset='train'):\n",
    "        sentence_a, sentence_b, labels = [], [], []\n",
    "        with open(file_path, 'r', encoding='utf8') as f:\n",
    "            for line in tqdm(f.readlines(), desc=f'Reading {dataset} data'):\n",
    "                line = json.loads(line)\n",
    "                sentence_a.append(line['sentence1'])\n",
    "                sentence_b.append(line['sentence2'])\n",
    "                if dataset != 'test':\n",
    "                    labels.append(int(line['label']))\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "\n",
    "                tokens = list(jieba.cut(sentence_a[-1])) + list(jieba.cut(sentence_b[-1]))\n",
    "                token_counter.update(tokens)\n",
    "        df = pd.DataFrame(zip(sentence_a, sentence_b, labels), columns=['text_a', 'text_b', 'labels'])\n",
    "        return df\n",
    "    \n",
    "    token_counter = Counter()\n",
    "    \n",
    "    train_df = convert2df(config['train_file_path'], 'train')\n",
    "    dev_df = convert2df(config['dev_file_path'], 'dev')\n",
    "    test_df = convert2df(config['test_file_path'], 'test')\n",
    "    \n",
    "    train_df = train_df.append(dev_df)\n",
    "    vocab = set(token for token, _ in token_counter.most_common(config['vocab_size']))\n",
    "    return train_df, test_df, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d98c3793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bbcaabd58d455b9225438cb6fae1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading train data:   0%|          | 0/34334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.789 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b24b635416243538a7277da60869bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dev data:   0%|          | 0/4316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7bd2e42fcc4eee918c242b099a86e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading test data:   0%|          | 0/3861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df, test_df, vocab = preprocess(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "789a3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词表对应成向量\n",
    "def get_embedding(vocab, embedding_file_path):\n",
    "    print('Processing embedding file...')\n",
    "    token2embedding = {}\n",
    "    \n",
    "    with bz2.open(embedding_file_path) as f:\n",
    "        token_vectors = f.readlines()\n",
    "        meta_info = token_vectors[0].split()\n",
    "        print(f'{int(meta_info[0])} tokens in embedding file in total, vector size is {int(meta_info[-1])}')\n",
    "        \n",
    "        for line in tqdm(token_vectors[1:]):\n",
    "            line = line.split()\n",
    "            token = line[0].decode('utf-8')\n",
    "            vector = line[1:]\n",
    "            if token in vocab:\n",
    "                token2embedding[token] = [float(num) for num in vector]\n",
    "    \n",
    "    token2idx = {token: idx for idx, token in enumerate(token2embedding.keys(), 4)}\n",
    "    UNK, PAD, BOS, EOS = '<unk>', '<pad>', '<bos>', '<eos>'\n",
    "    token2idx[PAD] = 0\n",
    "    token2idx[UNK] = 1\n",
    "    token2idx[BOS] = 2\n",
    "    token2idx[EOS] = 3\n",
    "    \n",
    "    idx2token = {idx: token for token, idx in token2idx.items()}\n",
    "    idx2embedding = {token2idx[token]: embedding for token, embedding in token2embedding.items()}\n",
    "    idx2embedding[0] = [.0] * int(meta_info[-1])\n",
    "    idx2embedding[1] = [.0] * int(meta_info[-1])\n",
    "    idx2embedding[2] = np.random.random(int(meta_info[-1])).tolist()\n",
    "    idx2embedding[3] = np.random.random(int(meta_info[-1])).tolist()\n",
    "    emb_mat = [idx2embedding[idx] for idx in range(len(idx2embedding))]\n",
    "\n",
    "    return torch.tensor(emb_mat, dtype=torch.float), token2idx, len(vocab) + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfb82395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing embedding file...\n",
      "195202 tokens in embedding file in total, vector size is 300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9851a7a9486445cd8ffe7a0294b20bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/195202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_matrix, token2idx, config['vocab_size'] = get_embedding(vocab, config['embedding_file_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63542e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def tokenizer(sent, token2id):\n",
    "    # not found -> 1 -> UNK\n",
    "    ids = [token2id.get(token, 1) for token in jieba.cut(sent)]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46ff69c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_df, train_val_ratio, token2id, mode='train'):\n",
    "    if mode == 'train':\n",
    "        X_train, y_train = defaultdict(list), []\n",
    "        X_val, y_val = defaultdict(list), []\n",
    "        num_val = int(len(data_df) * train_val_ratio)\n",
    "    else:\n",
    "        X_test, y_test = defaultdict(list), []\n",
    "        \n",
    "    for i, row in tqdm(data_df.iterrows(), desc=f'Precessing {mode} data', total=len(data_df)):\n",
    "        text_left, text_right, label = row[0], row[1], row[2]\n",
    "        \n",
    "        inputs_a = tokenizer(text_left, token2id=token2idx)\n",
    "        inputs_b = tokenizer(text_right, token2id=token2idx)\n",
    "        \n",
    "        if mode == 'train':\n",
    "            if i<num_val:\n",
    "                X_val['text_left'].append(inputs_a)\n",
    "                X_val['text_right'].append(inputs_b)\n",
    "                y_val.append(label)\n",
    "            else:\n",
    "                X_train['text_left'].append(inputs_a)\n",
    "                X_train['text_right'].append(inputs_b)\n",
    "                y_train.append(label)  \n",
    "\n",
    "        else:\n",
    "            X_test['text_left'].append(inputs_a)\n",
    "            X_test['text_right'].append(inputs_b)\n",
    "            y_test.append(label)\n",
    "            \n",
    "    if mode == 'train':\n",
    "        label2id = {label: i for i, label in enumerate(np.unique(y_train))}\n",
    "        id2label = {i: label for label, i in label2id.items()}\n",
    "        y_train = torch.tensor([label2id[label] for label in y_train], dtype=torch.long)\n",
    "        y_val = torch.tensor([label2id[label] for label in y_val], dtype=torch.long)\n",
    "        return X_train, y_train, X_val, y_val, label2id, id2label\n",
    "    else:\n",
    "        y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "        return X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d9dd168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0b667c46734f53a54edab5ffc55788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Precessing train data:   0%|          | 0/38650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bc1c4f254b4b119a581b66fa8c472d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Precessing test data:   0%|          | 0/3861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, label2id, id2label = read_data(train_df, config['train_val_ratio'], token2idx, mode='train')\n",
    "X_test, y_test = read_data(test_df, config['train_val_ratio'], token2idx, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "856f8cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AFQMCDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(AFQMCDataset, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        example = (self.x['text_left'][idx], self.x['text_right'][idx], self.y[idx])\n",
    "        return example\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.y.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44977622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 2, 3], [4, 5, 6], 1) ([777, 888, 999], [321, 543, 654], 0)\n",
      "<zip object at 0x7f162091d800>\n",
      "[([1, 2, 3], [777, 888, 999]), ([4, 5, 6], [321, 543, 654]), (1, 0)]\n"
     ]
    }
   ],
   "source": [
    "# 假设 传入 Collator一共两条数据\n",
    "# 每条数据 data = (sentence1, sentence2, label])\n",
    "datas = [([1,2,3],[4,5,6],1),([777,888,999],[321,543,654],0)]\n",
    "print(*datas)\n",
    "print(zip(*datas))\n",
    "print(list(zip(*datas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7de1a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "从 AFQMCDataset输出的example=(sent1, sent2, label)\n",
    "1. 将sent1, sent2, label分别放在一起\n",
    "2. 对齐操作, 按sent1, sent2中最长的句子\n",
    "3. 放入tensor\n",
    "\"\"\"\n",
    "class Collator:\n",
    "    def __init__(self, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def get_max_seq_len(self, ids_list):\n",
    "        cur_max_seq_len = max(len(input_id) for input_id in ids_list)\n",
    "        max_seq_len = min(self.max_seq_len, cur_max_seq_len)\n",
    "        return max_seq_len\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_and_truncate(text_ids_list, max_seq_len):\n",
    "        input_ids = torch.zeros((len(text_ids_list), max_seq_len), dtype=torch.long)\n",
    "        for i, text_ids in enumerate(text_ids_list):\n",
    "            seq_len = min(len(text_ids), max_seq_len)\n",
    "            input_ids[i, :seq_len] = torch.tensor(text_ids[:seq_len], dtype=torch.long)\n",
    "        \n",
    "        return input_ids\n",
    "    \n",
    "    def __call__(self, examples):\n",
    "        # step 1\n",
    "        text_ids_left_list, text_ids_right_list, labels_list = list(zip(*examples))\n",
    "        # step 2.1\n",
    "        max_text_left_length = self.get_max_seq_len(text_ids_left_list)\n",
    "        max_text_right_length = self.get_max_seq_len(text_ids_right_list)\n",
    "        # step 2.2\n",
    "        text_left_ids = self.pad_and_truncate(text_ids_left_list, max_text_left_length)\n",
    "        text_right_ids = self.pad_and_truncate(text_ids_right_list, max_text_right_length)\n",
    "        labels = torch.tensor(labels_list, dtype=torch.long)\n",
    "        \n",
    "        data_list = [text_left_ids, text_right_ids, labels]\n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc8da503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def build_dataloader(train_df, test_df, config, vocab):\n",
    "    X_train, y_train, X_val, y_val, label2id, id2label = read_data(train_df, config['train_val_ratio'], vocab, mode='train')\n",
    "    X_test, y_test = read_data(test_df, config['train_val_ratio'], vocab, mode='test')\n",
    "\n",
    "    train_dataset = AFQMCDataset(X_train, y_train)\n",
    "    val_dataset = AFQMCDataset(X_val, y_val)\n",
    "    test_dataset = AFQMCDataset(X_test, y_test)\n",
    "    \n",
    "    collate_fn = Collator(config['max_seq_len'])\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=config['batch_size'],\n",
    "                                  num_workers=4, shuffle=True, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=config['batch_size'],\n",
    "                                num_workers=4, shuffle=False, collate_fn=collate_fn)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=config['batch_size'],\n",
    "                                 num_workers=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return id2label, test_dataloader, train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1b16eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e75b58ca48478f99564c05f76d235f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Precessing train data:   0%|          | 0/38650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54bb7fff209340bf9c99dceb0bcefa8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Precessing test data:   0%|          | 0/3861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id2label, test_dataloader, train_dataloader, val_dataloader = build_dataloader(train_df, test_df, config, token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "543941af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 128,    1,  410,  ...,    0,    0,    0],\n",
      "        [ 272, 1359, 2930,  ...,    0,    0,    0],\n",
      "        [ 272, 1359, 3507,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 286,  894,   61,  ...,    0,    0,    0],\n",
      "        [ 746, 1313,  272,  ...,    0,    0,    0],\n",
      "        [ 117,   22,  286,  ...,    0,    0,    0]]), tensor([[ 410,  272, 1359, 4472, 3341,   20,  260,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 190, 3951,  272, 1359,  453, 2844,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  60, 1191,    1,   69, 1310,  272, 1359, 4369,   45,   88,    0,    0,\n",
      "            0,    0],\n",
      "        [ 272, 1359,   85,   45,  648,    4, 1448, 1359,  449,  180,   86,    0,\n",
      "            0,    0],\n",
      "        [2493, 1448, 1359,   61,   20, 4369, 4101,   88,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 272, 1359, 2076,   89,   16,  220,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  22,    5, 1663, 3075,  378, 1090, 1090, 1090,   61, 1793, 1448, 1359,\n",
      "           88,    0],\n",
      "        [ 272, 1359, 1715, 3341,  108,  150,  128,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [1448, 1359,    5,  232,    1,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 762, 2095,  438,   14, 1621,  272, 1359, 3940,   20,  260,    0,    0,\n",
      "            0,    0],\n",
      "        [2493, 1448, 1359,    4, 4101,   95,   13,  108,  150,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 272, 1359,  980,  125,    5,  611, 2506,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   1,  111,   22, 1448, 1359,    5, 3341,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 272, 1359, 3341, 1204,   89,   52,  556,  108,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [1448, 1359,   61,  416,   45,   88,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [2493, 1448, 1359,   20,  974, 3149, 2419,    9,    4, 1541,    0,    0,\n",
      "            0,    0],\n",
      "        [1448, 1359, 4664, 1236,  190,  180, 1546,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 272, 1359, 4101,   89,    4, 2418, 2791,   85, 1146,   13, 2797,    0,\n",
      "            0,    0],\n",
      "        [  22,    5,  272, 1359,  100,  375,   77, 2672, 1994,    4,    1,  190,\n",
      "         1994,    0],\n",
      "        [ 272, 1359, 3542,   89,   45,   61,   86,  240,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 190,  393, 4310, 1359,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   1,  272, 1359, 1715, 3341,  836,  346,   61, 4369, 4101,   88,    0,\n",
      "            0,    0],\n",
      "        [   1, 1313,  272, 1359,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  22, 1448, 1359,  378,    1, 1448,    5, 3507,  190, 2439, 1451,    0,\n",
      "            0,    0],\n",
      "        [ 410,    1,  180,    1, 1359,    9,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 410,   22, 1448, 1359, 4101,    9,   85, 1327, 3341,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 130,  272, 1359, 1254, 2035, 1104,   88,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [1448, 1359, 1448, 1359,    4,   22,   17, 4468, 4101,    9,    4,  410,\n",
      "           79, 1606],\n",
      "        [  60,  407,   22,  902,  167,  272, 1359, 3341,   88,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  86, 2791,  920,  379,  272, 1359, 4101,    4,   34, 1807,   88,    0,\n",
      "            0,    0],\n",
      "        [ 831, 1448, 1359,  143,   61,   86,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  22,    5, 2493,  272, 1359,  410,  456,   16,  762,   86,    0,    0,\n",
      "            0,    0],\n",
      "        [  22,    5, 1448, 1359, 3064, 3341,  190, 1462,    9,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [2493, 1448, 1359, 1378, 1448,  108,  150,   55, 3371,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   1,    5, 3341,   14, 1448, 1359, 3341,   52,  260,   88,    0,    0,\n",
      "            0,    0],\n",
      "        [ 272, 1359, 1715, 3341, 1253, 1635,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 272, 1359,   20,  276, 3232, 1034,   88,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 272, 1359,   60,  180,   16, 3083,  470,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [1448, 1359,    5,  607,   61, 1916,   57,   88,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 410,   22,   27,  272, 1359,   45,  220,  663,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 410,   22,    5, 1448, 1359, 4101,   89,  980,  233, 2418, 2461,    0,\n",
      "            0,    0],\n",
      "        [   1,   14, 1448, 1359,   13,  137,    5,   88,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 272, 1359, 3507,   45, 1726, 4101,   13,  190,    1,    5,    0,    0,\n",
      "            0,    0],\n",
      "        [1448, 1359,   45,   61, 3381,   88,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 272, 1359, 2791,  686,   61, 3303,  470,  240,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 272, 1359, 4369,   54, 1765,   13,    1,   88,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 272, 1359, 4369,    4, 3211,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  22,  100, 2534, 1859,    5,  272, 1359,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 272, 1359, 1715, 3341, 1626,  934,    1,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [1448, 1359,    1, 1527, 1635,   89,    4,  190,  134,  393, 3677,    0,\n",
      "            0,    0],\n",
      "        [ 190,  272, 1359, 1606,   22,  607,    9,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [2493, 1448, 1359,    1,  980, 3999, 2418, 3341,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 272, 1359, 1807,    9, 3004,  190,  104, 1104,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [2715, 5112, 1359, 2791, 1521,  190,   45,   60, 2715,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [4101,   95, 1378, 1448, 1359, 3381,   13,  111,   69,   45,   88,    0,\n",
      "            0,    0],\n",
      "        [ 131,   51, 1297,  180,  470,  272, 1359,    9,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   1, 1359, 2715,    4, 3211,   89,   95, 1981,   55,  272, 1359,   82,\n",
      "           88,    0],\n",
      "        [   1, 1359, 1254, 2035,  410,   85,   55, 3371,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  22, 3279,    5, 1859, 2076,    9,  272, 1359,    1,  130,   77,  190,\n",
      "         1313,    0],\n",
      "        [ 410,  272, 1359,   86,  663,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 272, 1359,    5,   75,    5, 1715, 3341,   60,  180, 4369,   45,    0,\n",
      "            0,    0],\n",
      "        [ 272, 1359,  190,   20,   61,  286, 3541,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  22, 1313,  272, 1359,    4,  114,  190, 1226,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  22,   13,   86,    5, 2493,  272, 1359, 1248,    5,    0,    0,    0,\n",
      "            0,    0]]), tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1])]\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataloader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2d681ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def evaluation(config, model, val_dataloader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    val_loss = 0.\n",
    "    val_iterator = tqdm(val_dataloader, desc='Evaluation', total=len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        for batch in val_iterator:\n",
    "            # batch [([sent_left_id1], [sent_left_id1],...), ([sent_right_id1], [sent_right_id2], ...), ([label], [label], ...)]\n",
    "            labels.append(batch[-1])\n",
    "            batch = [item.to(config['device']) for item in batch]\n",
    "            loss, logits = model(batch)[:2]\n",
    "            val_loss += loss.item()\n",
    "            preds.append(logits.argmax(dim=-1).detach().cpu())\n",
    "            \n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    labels = torch.cat(labels, dim=0).numpy()\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    return avg_val_loss, f1, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3dda497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def train(model, config, id2label, train_dataloader, val_dataloader):\n",
    "    optimizer = AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "    model.to(config['device'])\n",
    "    epoch_iterator = trange(config['num_epochs'])\n",
    "    \n",
    "    global_steps, train_loss, logging_loss = 0, 0., 0.\n",
    "    \n",
    "    for epoch in epoch_iterator:\n",
    "        train_iterator = tqdm(train_dataloader, desc='Training', total=len(train_dataloader))\n",
    "        model.train()\n",
    "        for batch in train_iterator:\n",
    "            batch = [item.to(config['device']) for item in batch]\n",
    "            loss = model(batch)[0]\n",
    "            \n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            global_steps += 1\n",
    "            \n",
    "            if global_steps % config['logging_step'] == 0:\n",
    "                print_train_loss = (train_loss - logging_loss) / config['logging_step']\n",
    "                logging_loss = train_loss\n",
    "                avg_val_loss, f1, acc = evaluation(config, model, val_dataloader)\n",
    "                \n",
    "                print_log = f'>>> training loss: {print_train_loss:.4f}, valid loss: {avg_val_loss:.4f}, ' \\\n",
    "                            f'valid f1 score: {f1:.4f}, valid acc: {acc:.4f}'\n",
    "                print(print_log)\n",
    "                model.train()\n",
    "\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e15d299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(config, id2label, model, test_dataloader):\n",
    "    test_iterator = tqdm(test_dataloader, desc='Predicting', total=len(test_dataloader))\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iterator:\n",
    "            batch = [item.to(config['device']) for item in batch]\n",
    "            logits = model(batch)[1]\n",
    "            test_preds.append(logits.argmax(dim=-1).detach().cpu())\n",
    "    test_preds = torch.cat(test_preds, dim=0).numpy()\n",
    "    test_preds = [id2label[id_] for id_ in test_preds]\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1c0426f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义一个 [2, 3, 4] -> [batch_size, seq_len, embedding_dim] 表示1个batch2个句子, 每个句子3个词每个词维度4\n",
    "a = torch.tensor([[[5., 5., 5., 5.],[6., 6., 6., 6.],[7., 7., 7., 7.]], [[1., 1., 1., 1.],[2., 2., 2., 2.],[3., 3., 3., 3.]]])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b829f9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [2, 1, 4]\n",
    "ones = torch.ByteTensor([[[1, 1, 0, 0]],[[0, 1, 1, 0]]])\n",
    "ones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2e91753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[5., 5., 5., 5.],\n",
      "         [6., 6., 6., 6.],\n",
      "         [7., 7., 7., 7.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [2., 2., 2., 2.],\n",
      "         [3., 3., 3., 3.]]])\n",
      "tensor([[[5., 5., 0., 0.],\n",
      "         [6., 6., 0., 0.],\n",
      "         [7., 7., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 1., 0.],\n",
      "         [0., 2., 2., 0.],\n",
      "         [0., 3., 3., 0.]]])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# 将词向量某些维度清0\n",
    "print(a)\n",
    "print(ones * a) # ones broadcast [2, 3, 4]\n",
    "print((ones * a).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b20243",
   "metadata": {},
   "source": [
    "#### masked_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9aabda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [2, 3, 1]\n",
    "mask = torch.BoolTensor([[[1], [1], [0]], [[0], [1], [1]]])\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1c3255d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义一个 [2, 3, 4] -> [batch_size, seq_len, embedding_dim] 表示1个batch2个句子, 每个句子3个词每个词维度4\n",
    "a = torch.tensor([[[5., 5., 5., 5.],[6., 6., 6., 6.],[7., 7., 7., 7.]], [[1., 1., 1., 1.],[2., 2., 2., 2.],[3., 3., 3., 3.]]])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "727d8d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[5., 5., 5., 5.],\n",
      "         [6., 6., 6., 6.],\n",
      "         [7., 7., 7., 7.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [2., 2., 2., 2.],\n",
      "         [3., 3., 3., 3.]]])\n",
      "tensor([[[-1.0000e+07, -1.0000e+07, -1.0000e+07, -1.0000e+07],\n",
      "         [-1.0000e+07, -1.0000e+07, -1.0000e+07, -1.0000e+07],\n",
      "         [ 7.0000e+00,  7.0000e+00,  7.0000e+00,  7.0000e+00]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
      "         [-1.0000e+07, -1.0000e+07, -1.0000e+07, -1.0000e+07],\n",
      "         [-1.0000e+07, -1.0000e+07, -1.0000e+07, -1.0000e+07]]])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# a [2, 3, 4]  mask [2, 3, 1] 可以广播\n",
    "# 将mask中为True的元素所在的索引, 在a中相同索引处替换为value\n",
    "b = a.masked_fill(mask, value=torch.tensor(-1e7))\n",
    "print(a)\n",
    "print(b)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12406ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [2, 1, 4]\n",
    "ones = torch.BoolTensor([[[1, 1, 0, 0]], [[0, 1, 1, 0]]])\n",
    "ones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ba7853e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0000e+07, -1.0000e+07,  5.0000e+00,  5.0000e+00],\n",
      "         [-1.0000e+07, -1.0000e+07,  6.0000e+00,  6.0000e+00],\n",
      "         [-1.0000e+07, -1.0000e+07,  7.0000e+00,  7.0000e+00]],\n",
      "\n",
      "        [[ 1.0000e+00, -1.0000e+07, -1.0000e+07,  1.0000e+00],\n",
      "         [ 2.0000e+00, -1.0000e+07, -1.0000e+07,  2.0000e+00],\n",
      "         [ 3.0000e+00, -1.0000e+07, -1.0000e+07,  3.0000e+00]]])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "mask = ones\n",
    "b = a.masked_fill(mask, value=torch.tensor(-1e7))\n",
    "print(b)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faebe731",
   "metadata": {},
   "source": [
    "#### ESIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "edb8ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'embedding': embedding_matrix,\n",
    "    'freeze_emb': True,\n",
    "    'hidden_size': 256,\n",
    "    'dropout': 0.3,\n",
    "    'num_layers': 2,\n",
    "    'concat_layers': True,\n",
    "    'rnn_type': 'lstm',\n",
    "    'num_labels': len(id2label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b53b215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c40c85d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDropout(nn.Dropout):\n",
    "    # 将词向量某些维度清0 [B, L, D]\n",
    "    def forward(self, sequence_batch):\n",
    "        # ones [B, D]\n",
    "        ones = sequence_batch.data.new_ones(sequence_batch.shape[0], sequence_batch.shape[-1])\n",
    "        # 随机mask [B, D]\n",
    "        dropout_mask = F.dropout(ones, self.p, self.training, inplace=False)\n",
    "        \n",
    "        return dropout_mask.unsqueeze(1) * sequence_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "242f93fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedBRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers,\n",
    "                 dropout_rate=0, dropout_output=False, rnn_type=nn.LSTM,\n",
    "                 concat_layers=False):\n",
    "        super(StackedBRNN, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout_output = dropout_output\n",
    "        self.num_layers = num_layers\n",
    "        self.concat_layers = concat_layers\n",
    "        self.rnns = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            input_size = input_size if i == 0 else 2 * hidden_size\n",
    "            self.rnns.append(rnn_type(input_size, hidden_size, num_layers=1, bidirectional=True))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # x [B, L, D] -> [L, B, D]\n",
    "        x = x.transpose(0, 1)\n",
    "        outputs = [x]\n",
    "        for i in range(self.num_layers):\n",
    "            rnn_input = outputs[-1]\n",
    "            if self.dropout_rate > 0:\n",
    "                rnn_input = F.dropout(rnn_input, p=self.dropout_rate, training=self.training)\n",
    "            # self.rnn[i](rnn_input) (output, (h_n, c_n))\n",
    "            rnn_output = self.rnns[i](rnn_input)[0]\n",
    "            outputs.append(rnn_output)\n",
    "            \n",
    "        # outputs: [x, output0, output1]\n",
    "        if self.concat_layers:\n",
    "            output = torch.cat(outputs[1:], 2)\n",
    "        else:\n",
    "            output = output[-1]\n",
    "        \n",
    "        # output [L, B, D] -> [B, L, D]\n",
    "        output = output.transpose(0, 1)\n",
    "        if self.dropout_output and self.dropout_rate:\n",
    "            output = F.dropout(output, p=self.dropout_rate, training=self.training)\n",
    "            \n",
    "        return output.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "635d5234",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BidirectionalAttention, self).__init__()\n",
    "\n",
    "    def forward(self, v1, v1_mask, v2, v2_mask):\n",
    "        # v1(a) [B, L, H]  v1_mask [B, L]\n",
    "        # v2(b) [B, R, H]  v2_mask [B, R]\n",
    "        # step 1 计算矩阵相似度 similarity_matrix [B, L, R]\n",
    "        similarity_matrix = v1.bmm(v2.transpose(2, 1).contiguous())\n",
    "        # step 2 mask step 3 softmax\n",
    "        # 将similarity_matxrix v1中pad对应的权重给mask\n",
    "        # [B, L, R]\n",
    "        v2_v1_attn = F.softmax(similarity_matrix.masked_fill(v1_mask.unsqueeze(2), -1e7), dim=1)\n",
    "        # 将similarity_matxrix v2中pad对应的权重给mask\n",
    "        # [B, L, R]\n",
    "        v1_v2_attn = F.softmax(similarity_matrix.masked_fill(v2_mask.unsqueeze(1), -1e7), dim=2)\n",
    "        # 计算attention\n",
    "        # [B, L, R] @ [B, R, H]\n",
    "        # 句子a对句子b的影响 [B, L, H]\n",
    "        attented_v1 = v1_v2_attn.bmm(v2)\n",
    "        # [B, L, R] -> [B, R, L] @ [B, L, H]\n",
    "        # 句子b对句子a的影响 [B, R, H]\n",
    "        attented_v2 = v2_v1_attn.transpose(1, 2).bmm(v1)\n",
    "        \n",
    "        # attented_v1将v1对应的pad填充为0\n",
    "        # attented_v2将v2对应的pad填充为0\n",
    "        attented_v1.masked_fill(v1_mask.unsqueeze(2), 0)\n",
    "        attented_v2.masked_fill(v2_mask.unsqueeze(2), 0)\n",
    "        \n",
    "        return attented_v1, attented_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fbf68abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESIM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ESIM, self).__init__()\n",
    "        rnn_mapping = {'lstm': nn.LSTM, 'gru': nn.GRU}\n",
    "        self.embedding = nn.Embedding.from_pretrained(config['embedding'], freeze=config['freeze_emb'])\n",
    "        self.rnn_dropout = RNNDropout(p=config['dropout'])\n",
    "        \n",
    "        rnn_size = config['hidden_size']\n",
    "        if config['concat_layers']:\n",
    "            rnn_size //= config['num_layers']\n",
    "            \n",
    "        self.input_encoding = StackedBRNN(input_size=config['embedding'].size(1),\n",
    "                                          hidden_size=rnn_size // 2,\n",
    "                                          num_layers=config['num_layers'],\n",
    "                                          rnn_type=rnn_mapping[config['rnn_type']],\n",
    "                                          concat_layers=config['concat_layers'])\n",
    "        \n",
    "        self.attention = BidirectionalAttention()\n",
    "        \n",
    "        self.projection = nn.Sequential(nn.Linear(4 * config['hidden_size'],\n",
    "                                        config['hidden_size']), nn.ReLU())\n",
    "        \n",
    "        self.composition = StackedBRNN(input_size=config['hidden_size'],\n",
    "                                       hidden_size=rnn_size // 2,\n",
    "                                       num_layers=config['num_layers'],\n",
    "                                       rnn_type=rnn_mapping[config['rnn_type']],\n",
    "                                       concat_layers=config['concat_layers'])\n",
    "        \n",
    "        self.classification = nn.Sequential(nn.Dropout(p=config['dropout']),\n",
    "                                            nn.Linear(4 * config['hidden_size'], config['hidden_size']),\n",
    "                                            nn.Tanh(),\n",
    "                                            nn.Dropout(p=config['dropout']))\n",
    "        self.out = nn.Linear(config['hidden_size'], config['num_labels'])\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # inputs: [sent1_tensor, sent2_tensor, labels_tensor]\n",
    "        # B: batch_size  L: inputs_left seq_len  R: inputs_right seq_len  D: embedding_size  H: hidden_size\n",
    "        # query: sent1_tensor [B, L]  doc: sent2_tensor [B, R]\n",
    "        query, doc = inputs[0].long(), inputs[1].long()\n",
    "        # 判断query和doc中的每一个数是不是0 query: [2, 3, 4, 5, 0, 0, 0] -> query_mask: [0, 0, 0, 0, 1, 1, 1]\n",
    "        query_mask, doc_mask = (query == 0), (doc == 0)\n",
    "        # query [B, L, D]  doc [B, R, D]\n",
    "        query, doc = self.embedding(query), self.embedding(doc)\n",
    "        # query [B, L, D]  doc [B, R, D]\n",
    "        query, doc = self.rnn_dropout(query), self.rnn_dropout(doc)\n",
    "        # query [B, L, H]  doc [B, R, H]\n",
    "        query, doc = self.input_encoding(query), self.input_encoding(doc)\n",
    "        # 1. 计算矩阵相似度 2. mask操作 3. 进行softmax 4. 计算attention\n",
    "        # query [B, L, H]  query_mask [B, L]\n",
    "        # doc [B, R, H]  doc_mask [B, R]\n",
    "        attended_query, attened_doc = self.attention(query, query_mask, doc, doc_mask)\n",
    "        # Enhanced 拼接\n",
    "        # enhanced_query [B, L, 4 * h]\n",
    "        # enhanced_doc [B, R, 4 * h]\n",
    "        enhanced_query = torch.cat([query,\n",
    "                                    attended_query,\n",
    "                                    query - attended_query,\n",
    "                                    query * attended_query], dim=-1)\n",
    "        enhanced_doc = torch.cat([doc,\n",
    "                                  attened_doc,\n",
    "                                  doc - attened_doc,\n",
    "                                  doc * attened_doc], dim=-1)\n",
    "        # projected_query [B, L, H]\n",
    "        # projected_doc [B, R, H]\n",
    "        projected_query, projected_doc = self.projection(enhanced_query), self.projection(enhanced_doc)\n",
    "        # query [B, L, H]\n",
    "        # doc [B, R, H]\n",
    "        query, doc = self.composition(projected_query), self.composition(projected_doc)\n",
    "        # pooling\n",
    "        # query_mask, doc_mask 判断query和doc中的每一个数是不是0 如果是1表示该位置是pad\n",
    "        # reversed_query_mask 0代表pad\n",
    "        # reversed_query_mask [B, L]  reversed_doc_mask [B, R]\n",
    "        reversed_query_mask, reversed_doc_mask = 1. - query_mask.float(), 1. - doc_mask.float()\n",
    "        \n",
    "        query_avg = torch.sum(query * reversed_query_mask.unsqueeze(2), dim=1) / (torch.sum(reversed_doc_mask, dim=1, keepdim=True) + 1e-8)\n",
    "        doc_avg = torch.sum(doc * reversed_doc_mask.unsqueeze(2), dim=1) / (torch.sum(reversed_doc_mask, dim=1, keepdim=True) + 1e-8)\n",
    "            \n",
    "        # 防止pad影响\n",
    "        query = query.masked_fill(query_mask.unsqueeze(2), -1e7)\n",
    "        doc = doc.masked_fill(doc_mask.unsqueeze(2), -1e7)\n",
    "        \n",
    "        query_max, _ = query.max(dim=1)\n",
    "        doc_max, _ = doc.max(dim=1)\n",
    "        \n",
    "        # v [B, 4 * H]\n",
    "        v = torch.cat([query_avg, query_max, doc_avg, doc_max], dim=-1)\n",
    "        \n",
    "        # hidden [B, H]\n",
    "        hidden = self.classification(v)\n",
    "        out = self.out(hidden)\n",
    "        outputs = (out,)\n",
    "        if len(inputs) == 3:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(out, inputs[-1])\n",
    "            outputs = (loss,) + outputs\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b9db1f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ESIM(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8aedf2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac8c77aaee54ca6a3208ff31d1c0f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7306a10c13408faa02503b9f71d912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463ccd32400748cfa8c90dbbeffedf4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> training loss: 0.6162, valid loss: 0.6374, valid f1 score: 0.3992, valid acc: 0.6644\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f34416a3284a20946c143ad1da4ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> training loss: 0.6095, valid loss: 0.6390, valid f1 score: 0.3992, valid acc: 0.6644\n"
     ]
    }
   ],
   "source": [
    "best_model = train(model, config, id2label, train_dataloader, val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
