{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d937ae4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "config = {\n",
    "    'train_file_path': 'data/train.json',\n",
    "    'dev_file_path': 'data/dev.json',\n",
    "    'test_file_path': 'data/test.json',\n",
    "    'output_path': '.',\n",
    "    'model_path': '../NLP_Project/dataset/BERT_model/',\n",
    "    'batch_size': 16,\n",
    "    'num_epochs': 1,\n",
    "    'max_seq_len': 64,\n",
    "    'learning_rate': 2e-5,\n",
    "    'eps': 0.1,\n",
    "    'alpha': 0.3,\n",
    "    'adv': 'fgm',\n",
    "    'warmup_ratio': 0.05,\n",
    "    'weight_decay': 0.01,\n",
    "    'use_bucket': True,\n",
    "    'bucket_multiplier': 200,\n",
    "    'device': 'cuda',\n",
    "    'n_gpus': 0,\n",
    "    'use_amp': True,\n",
    "    'logging_step': 400,\n",
    "    'ema_start_step': 500,\n",
    "    'ema_start': False,\n",
    "    'seed': 2021\n",
    "}\n",
    "if not torch.cuda.is_available():\n",
    "    config['device'] = 'cpu'\n",
    "else:\n",
    "    config['n_gpus'] = torch.cuda.device_count()\n",
    "    config['batch_size'] *= config['n_gpus']\n",
    "    \n",
    "if not os.path.exists(config['output_path']):\n",
    "    os.makedirs((config['output_path']))\n",
    "    \n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c740cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def parse_data(path, data_type='train'):\n",
    "    sentence_a, sentence_b, labels = [], [], []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f.readlines(), desc=f'Reading {data_type} data'):\n",
    "            line = json.loads(line)\n",
    "            sentence_a.append(line['sentence1'])\n",
    "            sentence_b.append(line['sentence2'])\n",
    "            if data_type != 'test':\n",
    "                labels.append(int(line['label']))\n",
    "            else:\n",
    "                labels.append(0)\n",
    "    df = pd.DataFrame(zip(sentence_a, sentence_b, labels), columns=['text_a', 'text_b', 'labels'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bec3762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs: defaultdict(list)\n",
    "def builder_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer):\n",
    "    # add_special_tokens [CLS] [SEP]\n",
    "    # return token_type_ids sentence_a 0, sentence_b 1\n",
    "    # return_attention_mask 不是pad的部分1 是pad的部分0\n",
    "    inputs_dict = tokenizer.encode_plus(sentence_a, sentence_b, add_special_tokens=True,\n",
    "                                        return_token_type_ids=True, return_attention_mask=True)\n",
    "    inputs['input_ids'].append(inputs_dict['input_ids'])\n",
    "    inputs['token_type_ids'].append(inputs_dict['token_type_ids'])\n",
    "    inputs['attention_mask'].append(inputs_dict['attention_mask'])\n",
    "    inputs['labels'].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b28ba43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def read_data(config, tokenizer):\n",
    "    train_df = parse_data(config['train_file_path'], data_type='train')\n",
    "    dev_df = parse_data(config['dev_file_path'], data_type='dev')\n",
    "    test_df = parse_data(config['test_file_path'], data_type='test')\n",
    "    \n",
    "    data_df = {'train': train_df, 'dev': dev_df, 'test': test_df}\n",
    "    # 保存BERT的输入\n",
    "    processed_data = {}\n",
    "    for data_type, df in data_df.items():\n",
    "        inputs = defaultdict(list)\n",
    "        for i, row in tqdm(df.iterrows(), desc=f'Preprocessing {data_type} data', total=len(df)):\n",
    "            sentence_a, sentence_b, label = row[0], row[1], row[2]\n",
    "            builder_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer)\n",
    "        processed_data[data_type] = inputs\n",
    "        \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb6c77e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f65b43b5eda460b9dc0137250897627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading train data:   0%|          | 0/34334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf4caa907cb4a28bd1c7e8490686999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dev data:   0%|          | 0/4316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161271f45f78461489810b7838d848e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading test data:   0%|          | 0/3861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c265e258e24cc3aefb6919fe56be09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing train data:   0%|          | 0/34334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d27faa5c3b43a88d86633de6f7320e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing dev data:   0%|          | 0/4316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6747f77585be43cfaa10cc24c8cfc9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing test data:   0%|          | 0/3861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(config['model_path'])\n",
    "dt = read_data(config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc05c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6010, 6009, 955, 1446, 5023, 7583, 6820, 3621, 1377, 809, 2940, 2768, 1044, 2622, 1400, 3315, 1408, 102, 955, 1446, 3300, 1044, 2622, 1168, 3309, 6820, 3315, 1408, 102]\n"
     ]
    }
   ],
   "source": [
    "print(dt['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "306822e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AFQMCDataset(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        super(AFQMCDataset, self).__init__()\n",
    "        self.data_dict = data_dict\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = (self.data_dict['input_ids'][idx],\n",
    "                   self.data_dict['token_type_ids'][idx],\n",
    "                   self.data_dict['attention_mask'][idx],\n",
    "                   self.data_dict['labels'][idx])\n",
    "        return example\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_dict['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f53f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    def __init__(self, max_seq_len, tokenizer):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def pad_and_truncate(self, input_ids_list, token_type_ids_list, attention_mask_list, labels_list, max_seq_len):\n",
    "        input_ids = torch.zeros((len(input_ids_list), max_seq_len), dtype=torch.long)\n",
    "        token_type_ids = torch.zeros_like(input_ids)\n",
    "        attention_mask = torch.zeros_like(input_ids)\n",
    "        \n",
    "        for i in range(len(input_ids_list)):\n",
    "            seq_len = len(input_ids_list[i])\n",
    "            \n",
    "            if seq_len <= max_seq_len:\n",
    "                input_ids[i, :seq_len] = torch.tensor(input_ids_list[i], dtype=torch.long)\n",
    "                token_type_ids[i, :seq_len] = torch.tensor(token_type_ids_list[i], dtype=torch.long)\n",
    "                attention_mask[i, :seq_len] = torch.tensor(attention_mask_list[i], dtype=torch.long)\n",
    "            else:\n",
    "                # input_ids 最后一位上放一个特殊的token\n",
    "                input_ids[i] = torch.tensor(input_ids_list[i][:max_seq_len-1] + [self.tokenizer.sep_token_id], dtype=torch.long)\n",
    "                token_type_ids[i] = torch.tensor(token_type_ids_list[i][:max_seq_len], dtype=torch.long)\n",
    "                attention_mask[i] = torch.tensor(attention_mask_list[i][:max_seq_len], dtype=torch.long)\n",
    "        labels = torch.tensor(labels_list, dtype=torch.long)\n",
    "        \n",
    "        return input_ids, token_type_ids, attention_mask, labels\n",
    "        \n",
    "    def __call__(self, examples):\n",
    "        input_ids_list, token_type_ids_list, attention_mask_list, labels_list = list(zip(*examples))\n",
    "        cur_max_seq_len = max(len(input_id) for input_id in input_ids_list)\n",
    "        max_seq_len = min(cur_max_seq_len, self.max_seq_len)\n",
    "        input_ids, token_type_ids, attention_mask, labels = self.pad_and_truncate(input_ids_list,\n",
    "                                                                                  token_type_ids_list,\n",
    "                                                                                  attention_mask_list,\n",
    "                                                                                  labels_list, max_seq_len)\n",
    "        data_dict = {\n",
    "            'input_ids': input_ids,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "        \n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4af37cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = Collator(config['max_seq_len'], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a52997",
   "metadata": {},
   "source": [
    "### Sampler Summary\n",
    "- 所有采样器都继承Sampler这个类\n",
    "- 需要实现`__iter__`, `__len__`方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfe4619",
   "metadata": {},
   "source": [
    "##### SequentialSampler\n",
    "- 在初始化时拿到数据集, 按顺序对元素采样, 每次只返回一个索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe2bb42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# 模拟数据 (2, 3, 4) -> batch_size 2, seq_len 3, embedding_dim 4 每个batch2条数据, 每个句子3个词, 每个词维度4\n",
    "a = torch.randperm(60).reshape(5, 3, 4)\n",
    "# print(a)\n",
    "b = torch.utils.data.SequentialSampler(a)\n",
    "for i in b:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6e1806",
   "metadata": {},
   "source": [
    "##### RandomSampler\n",
    "- replacement: True表示可以重复采样(类似有放回)\n",
    "- num_samples: 指定采样的数量\n",
    "- 当replacement=False时不应指定num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9e1b265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "a = torch.randperm(60).reshape((5, 3, 4))\n",
    "b = torch.utils.data.RandomSampler(a, replacement=True, num_samples=3)\n",
    "for i in b:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fca706",
   "metadata": {},
   "source": [
    "##### SubsetRandomSampler\n",
    "- Samples elements randomly from a given list of indices, without replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3175b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor(4)\n",
      "tensor(3)\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(5)\n",
    "print(a)\n",
    "b = torch.utils.data.SubsetRandomSampler(indices=a[2:])\n",
    "for i in b:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91572cb8",
   "metadata": {},
   "source": [
    "##### BatchSampler\n",
    "- sampler: 基采样器\n",
    "- batch_size\n",
    "- drop_last: True如果一个batch的长度小于batch_size则丢弃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db5e37cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "[0, 1]\n",
      "[2, 3]\n"
     ]
    }
   ],
   "source": [
    "a = torch.randperm(60).reshape((5, 3, 4))\n",
    "base_b = torch.utils.data.SequentialSampler(a)\n",
    "for i in base_b:\n",
    "    print(i)\n",
    "b = torch.utils.data.BatchSampler(base_b, 2, drop_last=True)\n",
    "for i in b:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694ef885",
   "metadata": {},
   "source": [
    "##### BucketBatchSampler\n",
    "- Dataset ->RandomSampler-> 关于Dataset的随机序列 ->BatchSampler-> min(n*batch_size, len(Dataset)) <br/> 得到bucket ->SortedSampler-> 得到bucket的排序索引 ->BatchSampler-> 得到若干batch_size大小的小bucket <br/> ->随机抽取小bucket->返回小bucket在大bucket中大位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "114e68c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source code\n",
    "import math\n",
    "from torch.utils.data import Sampler ,BatchSampler, RandomSampler, SubsetRandomSampler\n",
    "\n",
    "class SortedSampler(Sampler):\n",
    "    def __init__(self, data, sort_key):\n",
    "        super().__init__(data)\n",
    "        self.sort_key = sort_key\n",
    "        zip_ = [(i, self.sort_key(row)) for i, row in enumerate(self.data)]\n",
    "        zip_ = sorted(zip_, key=lambda r: r[1])\n",
    "        self.sorted_indice = [item[0] for item in zip_]\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return iter(self.sorted_indice)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "        \n",
    "class BucketBatchSampler(BatchSampler):\n",
    "    def __init__(self, sampler, batch_size, drop_last, sort_key, bucket_size_multiplier=100):\n",
    "        super().__init__(sampler, batch_size, drop_last)\n",
    "        self.sort_key = sort_key\n",
    "        self.bucket_sampler = BatchSampler(sampler, min(batch_size * bucket_size_multiplier, len(sampler)), False)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for bucket in self.bucket_sampler:\n",
    "            sorted_sampler = SortedSampler(bucket, self.sort_key)\n",
    "            for batch in SubsetRandomSampler(list(BatchSampler(sorted_sampler, self.batch_size, self.drop_last))):\n",
    "                yield [bucket[i] for i in batch]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler) // self.batch_size\n",
    "        else:\n",
    "            return math.ceil(len(self.sampler) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84afc3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "24\n",
      "25\n",
      "24\n",
      "59\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "mini_dataset = {k: v[:6] for k, v in dt['train'].items()}\n",
    "mini_data = AFQMCDataset(mini_dataset)\n",
    "# mini_data前6条数据\n",
    "for i, d in enumerate(mini_data):\n",
    "    print(len(d[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e4506f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3, 1, 0, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "from extra_file.bucket_sampler import SortedSampler\n",
    "\n",
    "random_sampler = RandomSampler(mini_data, replacement=False)\n",
    "print(list(random_sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0b9a613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 0, 1]\n",
      "[3, 1, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "batch_sampler = BatchSampler(random_sampler, 4, drop_last=True)\n",
    "for sample in batch_sampler:\n",
    "    print(sample)\n",
    "    sorted_sampler = SortedSampler(sample, sort_key=lambda x: len(mini_data[x][0]))\n",
    "    print(list(sorted_sampler))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08bb270",
   "metadata": {},
   "source": [
    "```\n",
    "30 24 25 24 59 28\n",
    "[1, 5, 2, 3]\n",
    "[0, 3, 2, 1]:\n",
    "    0->1->24\n",
    "    3->3->24\n",
    "    2->2->25\n",
    "    1->5->28\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72c1b42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 1], [2, 0]]\n"
     ]
    }
   ],
   "source": [
    "c = list(BatchSampler(sorted_sampler, 2, drop_last=True))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5d52745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从给定的索引列表中随机采样元素\n",
      "[3, 1]\n",
      "所对应的原序列是\n",
      "[1, 5]\n",
      "从给定的索引列表中随机采样元素\n",
      "[2, 0]\n",
      "所对应的原序列是\n",
      "[0, 4]\n"
     ]
    }
   ],
   "source": [
    "for batch in SubsetRandomSampler(c):\n",
    "    print('从给定的索引列表中随机采样元素')\n",
    "    print(batch)\n",
    "    print('所对应的原序列是')\n",
    "    print([sample[i] for i in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1530e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def build_dataloader(config, data, collate_fn):\n",
    "    train_dataset = AFQMCDataset(data['train'])\n",
    "    dev_dataset = AFQMCDataset(data['dev'])\n",
    "    test_dataset = AFQMCDataset(data['test'])\n",
    "    \n",
    "    if config['use_bucket']:\n",
    "        train_sampler = RandomSampler(train_dataset)\n",
    "        bucket_sampler = BucketBatchSampler(train_sampler, batch_size=config['batch_size'],\n",
    "                                            drop_last=True, sort_key=lambda x: len(train_dataset[x][0]),\n",
    "                                            bucket_size_multiplier=config['bucket_multiplier'])\n",
    "        train_dataloader = DataLoader(train_dataset, batch_sampler=bucket_sampler,\n",
    "                                      num_workers=4, collate_fn=collate_fn)\n",
    "    else:\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True,\n",
    "                                      num_workers=4, collate_fn=collate_fn)\n",
    "    \n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size=config['batch_size'], \n",
    "                                     shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "    \n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], \n",
    "                                     shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "    \n",
    "    return train_dataloader, dev_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "658c05aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, dev_dataloader, test_dataloader = build_dataloader(config, dt, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b314690b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 5709, 1446,  ..., 2512, 1510,  102],\n",
      "        [ 101, 2769, 4638,  ...,  809, 1408,  102],\n",
      "        [ 101,  711,  784,  ..., 4638, 1660,  102],\n",
      "        ...,\n",
      "        [ 101, 3221, 1728,  ..., 3326,  749,  102],\n",
      "        [ 101, 2769,  955,  ...,  955, 1446,  102],\n",
      "        [ 101, 3221,  679,  ..., 6820, 1408,  102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
      "        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for i in train_dataloader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc92e14",
   "metadata": {},
   "source": [
    "##### 混合精度训练\n",
    "\n",
    "作用: 训练时尽量不降低性能, 并提升速度 <br/>\n",
    "\n",
    "Float16优点:\n",
    "- 减少内存的使用\n",
    "- 加快训练和推理计算\n",
    "\n",
    "Float16缺点:\n",
    "- 溢出错误\n",
    "- 舍入误差\n",
    "\n",
    "当进入autocast()时, 系统自动切换为Float16, autocast上下文只包含前向传播\n",
    "\n",
    "scaler.scale(loss)将损失乘以缩放器当前比例因子, 进行反向传播\n",
    "\n",
    "scaler.step(optimizer)取消缩放梯度并调用optimizer.step()\n",
    "\n",
    "scaler.update()更新缩放器的比例因子\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4170a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def evaluation(config, model, val_dataloader):\n",
    "    model.eval()\n",
    "    preds, labels, val_loss = [], [], 0.\n",
    "    val_iterator = tqdm(val_dataloader, desc='Evaluation', total=len(val_dataloader))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_iterator:\n",
    "            labels.append(batch['labels'])\n",
    "            batch_cuda = {item: value.to(config['device']) for item, value in list(batch.items())}\n",
    "            loss, logits = model(**batch_cuda)[:2]\n",
    "            \n",
    "            if config['n_gpus'] > 1:\n",
    "                loss = loss.mean()\n",
    "                \n",
    "            val_loss += loss.item()\n",
    "            preds.append(logits.argmax(dim=-1).detach().cpu())\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    labels = torch.cat(labels, dim=0).numpy()\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    f1 = f1_score(labels, preds)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    return avg_val_loss, f1, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a4bf794",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    def __init__(self, model, decay):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        self.register()\n",
    "\n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                self.backup[name] = param.data\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d86ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from torch.cuda import amp\n",
    "from torch.optim import AdamW\n",
    "from extra_file.extra_pgd import *\n",
    "from extra_file.extra_fgm import *\n",
    "from extra_file.extra_loss import *\n",
    "from extra_file.extra_optim import *\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "def train(config, train_dataloader,dev_dataloader):\n",
    "    # 封装好 BertForSequenceClassification\n",
    "    model = BertForSequenceClassification.from_pretrained(config['model_path'])\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    # 实例化scaler对象使用梯度缩放\n",
    "    scaler = amp.GradScaler(enabled=config['use_amp'])\n",
    "    # 权重缩减\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    # ['bias', 'LayerNorm.weight']权重衰减因子为0\n",
    "    # 其它权重衰减因子为0.01\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weigth_decay': config['weight_decay']},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': 0.}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=config['learning_rate'], eps=1e-8)\n",
    "    # Lookahead 预先查看AdamW生成的快权重来选择搜索方向\n",
    "    optimizer = Lookahead(optimizer, 5, 1)\n",
    "    total_steps = config['num_epochs'] * len(train_dataloader)\n",
    "    # 使用warmup调整学习率\n",
    "    lr_scheduler = WarmupLinearSchedule(optimizer, warmup_steps=int(config['warmup_ratio'] * total_steps),\n",
    "                                        t_total=total_steps)\n",
    "    model.to(config['device'])\n",
    "    \n",
    "    if config['adv'] == 'fgm':\n",
    "        fgm = FGM(model)\n",
    "    else:\n",
    "        pgd = PGD(model)\n",
    "        K = 3\n",
    "        \n",
    "    epoch_iterator = trange(config['num_epochs'])\n",
    "    global_steps, train_loss, logging_loss, best_acc, best_model_path = 0, 0., 0., 0., ''\n",
    "    \n",
    "    if config['n_gpus'] > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        \n",
    "    for _ in epoch_iterator:\n",
    "        train_iterator = tqdm(train_dataloader, desc='Training', total=len(train_dataloader))\n",
    "        model.train()\n",
    "        for batch in train_iterator:\n",
    "            batch_cuda = {item: value.to(config['device']) for item, value in list(batch.items())}\n",
    "            # 前向传播\n",
    "            with amp.autocast(enabled=config['use_amp']):\n",
    "                loss = model(**batch_cuda)[0]\n",
    "                # 多卡取平均\n",
    "                if config['n_gpus'] > 1:\n",
    "                    loss = loss.mean()\n",
    "                    \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if config['adv'] == 'fgm':\n",
    "                # 在embedding上加扰动\n",
    "                fgm.attack(epsilon=config['eps'])\n",
    "                # autocast\n",
    "                with amp.autocast(enabled=config['use_amp']):\n",
    "                    loss_adv = model(**batch_cuda)[0]\n",
    "                    if config['n_gpus'] > 1:\n",
    "                        loss_adv = loss_adv.mean()\n",
    "                \n",
    "                scaler.scale(loss_adv).backward()\n",
    "                # 恢复embedding参数\n",
    "                fgm.restore()\n",
    "            else:\n",
    "                pgd.backup_grad()\n",
    "                for t in range(K):\n",
    "                    pgd.attack(epsilon=config['eps'], alpha=config['alpha'], is_first_attack=(t == 0))\n",
    "                    if t != K - 1:\n",
    "                        model.zero_grad()\n",
    "                    else:\n",
    "                        pgd.restore_grad()\n",
    "                    with amp.autocast(enabled=config['use_amp']):\n",
    "                        loss_adv = model(**batch_cuda)[0]\n",
    "                        if config['n_gpus'] > 1:\n",
    "                            loss_adv = loss_adv.mean()\n",
    "\n",
    "                    scaler.scale(loss_adv).backward()\n",
    "                pgd.restore()\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if config['ema_start']:\n",
    "                ema.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            global_steps += 1\n",
    "\n",
    "            train_iterator.set_postfix_str(f'running training loss: {loss.item():.4f}')\n",
    "\n",
    "            if global_steps % config['logging_step'] == 0:\n",
    "                if global_steps >= config['ema_start_step'] and not config['ema_start']:\n",
    "                    print('\\n>>> EMA starting ...')\n",
    "                    config['ema_start'] = True\n",
    "                    \n",
    "                    ema = EMA(model.module if hasattr(model, 'module') else model, decay=0.999)\n",
    "\n",
    "                print_train_loss = (train_loss - logging_loss) / config['logging_step']\n",
    "                logging_loss = train_loss\n",
    "\n",
    "\n",
    "                if config['ema_start']:\n",
    "                    ema.apply_shadow()\n",
    "                val_loss, f1, acc = evaluation(config, model, dev_dataloader)\n",
    "\n",
    "                print_log = f'\\n>>> training loss: {print_train_loss:.6f}, valid loss: {val_loss:.6f}, '\n",
    "\n",
    "                if acc > best_acc:\n",
    "                    model_save_path = os.path.join(config['output_path'],\n",
    "                                                   f'checkpoint-{global_steps}-{acc:.6f}')\n",
    "                    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "                    model_to_save.save_pretrained(model_save_path)\n",
    "                    best_acc = acc\n",
    "                    best_model_path = model_save_path\n",
    "\n",
    "                print_log += f'valid f1: {f1:.6f}, valid acc: {acc:.6f}'\n",
    "\n",
    "                print(print_log)\n",
    "                model.train()\n",
    "\n",
    "                if config['ema_start']:\n",
    "                    ema.restore()\n",
    "\n",
    "    return model, best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "feb2ed7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../NLP_Project/dataset/BERT_model/ were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../NLP_Project/dataset/BERT_model/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb040bc74a94e09982a452644409c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313b9eb446ce4c58a45533df1c9fa853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169eeb21e6054dbf9367205d1983ed80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> training loss: 0.582753, valid loss: 0.531598, valid f1: 0.483151, valid acc: 0.705051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DataParallel(\n",
       "   (module): BertForSequenceClassification(\n",
       "     (bert): BertModel(\n",
       "       (embeddings): BertEmbeddings(\n",
       "         (word_embeddings): Embedding(21128, 768, padding_idx=1)\n",
       "         (position_embeddings): Embedding(512, 768)\n",
       "         (token_type_embeddings): Embedding(2, 768)\n",
       "         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (encoder): BertEncoder(\n",
       "         (layer): ModuleList(\n",
       "           (0): BertLayer(\n",
       "             (attention): BertAttention(\n",
       "               (self): BertSelfAttention(\n",
       "                 (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "               (output): BertSelfOutput(\n",
       "                 (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (intermediate): BertIntermediate(\n",
       "               (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "               (intermediate_act_fn): GELUActivation()\n",
       "             )\n",
       "             (output): BertOutput(\n",
       "               (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (1): BertLayer(\n",
       "             (attention): BertAttention(\n",
       "               (self): BertSelfAttention(\n",
       "                 (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "               (output): BertSelfOutput(\n",
       "                 (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (intermediate): BertIntermediate(\n",
       "               (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "               (intermediate_act_fn): GELUActivation()\n",
       "             )\n",
       "             (output): BertOutput(\n",
       "               (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (2): BertLayer(\n",
       "             (attention): BertAttention(\n",
       "               (self): BertSelfAttention(\n",
       "                 (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "               (output): BertSelfOutput(\n",
       "                 (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (intermediate): BertIntermediate(\n",
       "               (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "               (intermediate_act_fn): GELUActivation()\n",
       "             )\n",
       "             (output): BertOutput(\n",
       "               (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (3): BertLayer(\n",
       "             (attention): BertAttention(\n",
       "               (self): BertSelfAttention(\n",
       "                 (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "               (output): BertSelfOutput(\n",
       "                 (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (intermediate): BertIntermediate(\n",
       "               (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "               (intermediate_act_fn): GELUActivation()\n",
       "             )\n",
       "             (output): BertOutput(\n",
       "               (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (4): BertLayer(\n",
       "             (attention): BertAttention(\n",
       "               (self): BertSelfAttention(\n",
       "                 (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "               (output): BertSelfOutput(\n",
       "                 (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (intermediate): BertIntermediate(\n",
       "               (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "               (intermediate_act_fn): GELUActivation()\n",
       "             )\n",
       "             (output): BertOutput(\n",
       "               (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (5): BertLayer(\n",
       "             (attention): BertAttention(\n",
       "               (self): BertSelfAttention(\n",
       "                 (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "               (output): BertSelfOutput(\n",
       "                 (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (intermediate): BertIntermediate(\n",
       "               (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "               (intermediate_act_fn): GELUActivation()\n",
       "             )\n",
       "             (output): BertOutput(\n",
       "               (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (6): BertLayer(\n",
       "             (attention): BertAttention(\n",
       "               (self): BertSelfAttention(\n",
       "                 (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "               (output): BertSelfOutput(\n",
       "                 (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (intermediate): BertIntermediate(\n",
       "               (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "               (intermediate_act_fn): GELUActivation()\n",
       "             )\n",
       "             (output): BertOutput(\n",
       "               (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (7): BertLayer(\n",
       "             (attention): BertAttention(\n",
       "               (self): BertSelfAttention(\n",
       "                 (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "               (output): BertSelfOutput(\n",
       "                 (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (intermediate): BertIntermediate(\n",
       "               (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "               (intermediate_act_fn): GELUActivation()\n",
       "             )\n",
       "             (output): BertOutput(\n",
       "               (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (8): BertLayer(\n",
       "             (attention): BertAttention(\n",
       "               (self): BertSelfAttention(\n",
       "                 (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "               (output): BertSelfOutput(\n",
       "                 (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (intermediate): BertIntermediate(\n",
       "               (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "               (intermediate_act_fn): GELUActivation()\n",
       "             )\n",
       "             (output): BertOutput(\n",
       "               (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (9): BertLayer(\n",
       "             (attention): BertAttention(\n",
       "               (self): BertSelfAttention(\n",
       "                 (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "               (output): BertSelfOutput(\n",
       "                 (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (intermediate): BertIntermediate(\n",
       "               (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "               (intermediate_act_fn): GELUActivation()\n",
       "             )\n",
       "             (output): BertOutput(\n",
       "               (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (10): BertLayer(\n",
       "             (attention): BertAttention(\n",
       "               (self): BertSelfAttention(\n",
       "                 (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "               (output): BertSelfOutput(\n",
       "                 (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (intermediate): BertIntermediate(\n",
       "               (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "               (intermediate_act_fn): GELUActivation()\n",
       "             )\n",
       "             (output): BertOutput(\n",
       "               (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (11): BertLayer(\n",
       "             (attention): BertAttention(\n",
       "               (self): BertSelfAttention(\n",
       "                 (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "               (output): BertSelfOutput(\n",
       "                 (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (intermediate): BertIntermediate(\n",
       "               (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "               (intermediate_act_fn): GELUActivation()\n",
       "             )\n",
       "             (output): BertOutput(\n",
       "               (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (pooler): BertPooler(\n",
       "         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "         (activation): Tanh()\n",
       "       )\n",
       "     )\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "   )\n",
       " ),\n",
       " './checkpoint-400-0.705051')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(config, train_dataloader, dev_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
