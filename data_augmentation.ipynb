{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb7616c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /data2/cuimengjun/anaconda3/envs/d2l/lib/python3.8/site-packages/synonyms/data/vocab.txt ...\n",
      "Loading model from cache /tmp/jieba.u1ee567e3d900769013ee0717e970f060.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Synonyms: v3.18.0, Project home: https://github.com/chatopera/Synonyms/\n",
      "\n",
      " Project Sponsored by Chatopera\n",
      "\n",
      "  deliver your chatbots with Chatopera Cloud Services --> https://bot.chatopera.com\n",
      "\n",
      ">> Synonyms load wordseg dict [/data2/cuimengjun/anaconda3/envs/d2l/lib/python3.8/site-packages/synonyms/data/vocab.txt] ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.215 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Synonyms on loading stopwords [/data2/cuimengjun/anaconda3/envs/d2l/lib/python3.8/site-packages/synonyms/data/stopwords.txt] ...\n",
      ">> Synonyms on loading vectors [/data2/cuimengjun/anaconda3/envs/d2l/lib/python3.8/site-packages/synonyms/data/words.vector.gz] ...\n"
     ]
    }
   ],
   "source": [
    "import synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112af3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['爸爸', '妈妈', '爷爷', '奶奶', '老婆', '姐姐', '爸妈', '老公', '老爸', '外婆'],\n",
       " [1.0,\n",
       "  0.9197904,\n",
       "  0.8369896,\n",
       "  0.83030784,\n",
       "  0.80404544,\n",
       "  0.79813045,\n",
       "  0.7979228,\n",
       "  0.7714372,\n",
       "  0.77116084,\n",
       "  0.7584483])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms.nearby(\"爸爸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "044911b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {word.strip() for word in open('data/baidu_stopwords.txt', 'r', encoding='utf-8').readlines()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf6658be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同义词替换(SR): 从句子中随机选择n个不是停用词的词 用随机选择的同义词之一替换这些单词中的每一个\n",
    "import random\n",
    "\n",
    "def get_synonyms(word):\n",
    "    sys = set(synonyms.nearby(word)[0])\n",
    "    if word in sys:\n",
    "        sys.remove(word)\n",
    "        \n",
    "    return list(sys)\n",
    "\n",
    "def synonym_replacement(words, n):\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "    random.shuffle(random_word_list)\n",
    "    \n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced > n:\n",
    "            break\n",
    "            \n",
    "    sentence = ' '.join(new_words)\n",
    "    new_words = sentence.split(' ')\n",
    "    \n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f2e7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机删除(RD): 以概率p随机删除句子中的每个单词\n",
    "\n",
    "def random_deletion(words, p):\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > p:\n",
    "            new_words.append(word)\n",
    "            \n",
    "    if len(new_words) == 0:\n",
    "        random_int = random.randint(0, len(words) - 1)\n",
    "        return [words[random_int]]\n",
    "    \n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c19abe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机交换(RS): 随机选择句子中的两个单词并交换他们的位置 做n次\n",
    "def swap_word(new_words):\n",
    "    random_idx_1 = random.randint(0, len(new_words) - 1)\n",
    "    random_idx_2 = random_idx_1\n",
    "    \n",
    "    cnt = 0\n",
    "    while random_idx_2 == random_idx_1:\n",
    "        random_idx_2 = random.randint(0, len(new_words) - 1)\n",
    "        cnt += 1\n",
    "        if cnt > 3:\n",
    "            return new_words\n",
    "        \n",
    "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
    "    \n",
    "    return new_words\n",
    "\n",
    "def random_swap(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        new_words = swap_word(new_words)\n",
    "    \n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd9e57be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随即插入(RI): 在句子中随机找到一个词, 并找出其同义词, 且该词不是停用词, 将该同义词插入句子的随机位置 做n次\n",
    "def add_word(new_words):\n",
    "    synonyms = []\n",
    "    cnt = 0\n",
    "    while len(synonyms) < 1:\n",
    "        random_word = new_words[random.randint(0, len(new_words) - 1)]\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        cnt += 1\n",
    "        if cnt >= 10:\n",
    "            return\n",
    "    \n",
    "def random_insert(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        add_word(new_words)\n",
    "        \n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e218fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
    "    words = synonyms.seg(sentence)[0]\n",
    "    num_words = len(words)\n",
    "    augmented_sentence = []\n",
    "    num_new_per_technique = int(num_aug / 4) + 1\n",
    "    \n",
    "    n_sr = max(1, (int)(alpha_sr * num_words))\n",
    "    n_ri = max(1, int(alpha_ri*num_words))\n",
    "    n_rs = max(1, int(alpha_rs*num_words))\n",
    "    \n",
    "    # sr\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = synonym_replacement(words, n_sr)\n",
    "        augmented_sentence.append(' '.join(a_words))\n",
    "    # ri\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_insert(words, n_ri)\n",
    "        augmented_sentence.append(' '.join(a_words))\n",
    "    # rs\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_swap(words, n_rs)\n",
    "        augmented_sentence.append(' '.join(a_words))\n",
    "    # rd\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_deletion(words, p_rd)\n",
    "        augmented_sentence.append(' '. join(a_words))\n",
    "        \n",
    "    random.shuffle(augmented_sentence)\n",
    "    if num_aug >= 1:\n",
    "        augmented_sentence = augmented_sentence[:num_aug]\n",
    "    else:\n",
    "        keep_prob = num_aug / len(augmented_sentence)\n",
    "        augmented_sentence = [s for s in augmented_sentence if random.uniform(0, 1) < keep_prob]\n",
    "    \n",
    "    return augmented_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeee8198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['目前', '华为', '部分', '型号', '的', '手机', '产品', '出现', '货', '少', '的', '现象'], ['t', 'nr', 'n', 'n', 'uj', 'n', 'n', 'v', 'n', 'a', 'uj', 'n'])\n"
     ]
    }
   ],
   "source": [
    "words = synonyms.seg('目前华为部分型号的手机产品出现货少的现象')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e06d4191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9 月 15 日 ， ， 台积电 下去 高通 、 三星 等 华为 的 重要 合作伙伴 ， 的 没有 美国 的 美国 许可证 芯片 都 无法 供应 ， 给 华为 ， 而 中芯国际 等 遭受 芯片 企业 ， 也 因 采用 相关 技术 以来 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 出现 产品 货 少 只要 现象 ， 若 该 形势 持续 、 ， 华为 手机 业务 将 国产 重创 。',\n",
       " '9 月 15 日 技术 的 台积电 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 目前 ， 都 无法 供应 持续 给 华为 ， 而 中芯国际 等 国产 芯片 企业 货 也 因 该 美国 以来 ， 而 无法 供货 给 华为 。 许可证 现象 部分 型号 的 手机 产品 出现 ， 少 ， 华为 ， 若 采用 形势 芯片 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
       " '9 日 ， 、 高通 三星 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 企业 ， 也 因 采用 美国 技术 ， 而 供货 给 华为 。 目前 华为 部分 型号 的 产品 出现 货 少 的 现象 ， 若 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
       " '9 月 日 15 以来 等 台积电 、 高通 手机 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 部分 相关 许可证 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 、 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 少 无法 供货 给 华为 。 目前 华为 的 型号 的 ， 产品 出现 货 而 的 现象 ， 若 遭受 形势 持续 下去 ， 华为 手机 业务 将 该 重创 。',\n",
       " '9 月 15 日 以来 ， 、 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 的 相关 许可证 ， 都 无法 供应 芯片 给 ， 而 中芯国际 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 产品 出现 货 少 的 现象 ， 若 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
       " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 分销商 ， 只要 没有 澳洲 的 相关 许可证 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 投放市场 芯片 企业 ， 也 因 采用 澳洲 技术 ， 而 无法 交货 给 华为 。 目前 华为 部分 基本型 的 手机 产品 出现 白米 相对来说 的 现象 ， 若 该 形势 持续 下去 ， 华为 手机 业务 将 遭受 侵袭 。',\n",
       " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 货 少 的 现象 ， 若 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
       " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货 少 的 现象 ， 若 该 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
       " '9 月前 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 该些 许可证 ， 都 无法 供应 芯片 给 华为 ， 而 Nashik 等 换代 芯片 跨国企业 ， 也 因 装配 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 笔记本 产品 出现 货 贵 的 现象 ， 若 该 形势 持续 下去 ， 华为 笔记本 业务 将 遭受 重创 。']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda('9月15日以来，台积电、高通、三星等华为的重要合作伙伴，只要没有美国的相关许可证，都无法供应芯片给华为，而中芯国际等国产芯片企业，也因采用美国技术，而无法供货给华为。目前华为部分型号的手机产品出现货少的现象，若该形势持续下去，华为手机业务将遭受重创。')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e96edb",
   "metadata": {},
   "source": [
    "#### 闭包数据增强\n",
    "数据集中每条数据有两个句子\\\n",
    "a和b相似 a和c相似\\\n",
    "则b和c相似\\\n",
    "a和d不相似 0\\\n",
    "则b和d不相似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64d3bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "def parse_data(path, data_type='train'):\n",
    "    sentence_a, sentence_b, labels = [], [], []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f.readlines(), desc=f'Reading {data_type} data'):\n",
    "            line = json.loads(line)\n",
    "            sentence_a.append(line['sentence1'])\n",
    "            sentence_b.append(line['sentence2'])\n",
    "            if data_type != 'test':\n",
    "                labels.append(int(line['label']))\n",
    "            else:\n",
    "                labels.append(0)\n",
    "    df = pd.DataFrame(zip(sentence_a, sentence_b, labels), columns=['text_a', 'text_b', 'labels'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6af9d0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de3bf2ad590459aba5c1b08394ee495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading train data:   0%|          | 0/34334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = parse_data('data/train.json', data_type='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8b5cb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>蚂蚁借呗等额还款可以换成先息后本吗</td>\n",
       "      <td>借呗有先息到期还本吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>蚂蚁花呗说我违约一次</td>\n",
       "      <td>蚂蚁花呗违约行为是什么</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>帮我看一下本月花呗账单有没有结清</td>\n",
       "      <td>下月花呗账单</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              text_a       text_b  labels\n",
       "0  蚂蚁借呗等额还款可以换成先息后本吗   借呗有先息到期还本吗       0\n",
       "1         蚂蚁花呗说我违约一次  蚂蚁花呗违约行为是什么       0\n",
       "2   帮我看一下本月花呗账单有没有结清       下月花呗账单       0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ee7f404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为何我的花呗不能用\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "          text_a       text_b  labels\n",
      "13425  为何我的花呗不能用     为什么花呗用不了       1\n",
      "14005  为何我的花呗不能用    花呗不能正常使用了       1\n",
      "15175  为何我的花呗不能用  我现在是不是用不了花呗       0\n"
     ]
    }
   ],
   "source": [
    "for data in train_df.groupby(by=['text_a']):\n",
    "    if len(data[1]) == 3:\n",
    "        print(data[0])\n",
    "        print(type(data[1]))\n",
    "        print(data[1])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "040e1495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def aug_group_by_a(df):\n",
    "    aug_data = defaultdict(list)\n",
    "    # 以text_a句子为a\n",
    "    for g, data in df.groupby(by=['text_a']):\n",
    "        if len(data) < 2:\n",
    "            continue\n",
    "        for i in range(len(data)):\n",
    "            for j in range(i + 1, len(data)):\n",
    "                # 取出b的值 a, b的label\n",
    "                row_i_text = data.iloc[i, 1]\n",
    "                row_i_label = data.iloc[i, 2]\n",
    "                # 取出c的值 a, c的label\n",
    "                row_j_text = data.iloc[j, 1]\n",
    "                row_j_label = data.iloc[j, 2]\n",
    "                \n",
    "                if row_i_label == row_j_label == 0:\n",
    "                    continue\n",
    "                aug_label = 1 if row_i_label == row_j_label == 1 else 0\n",
    "                \n",
    "                aug_data['text_a'].append(row_i_text)\n",
    "                aug_data['text_b'].append(row_j_text)\n",
    "                aug_data['labels'].append(aug_label)\n",
    "    \n",
    "    return pd.DataFrame(aug_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6555e7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228\n"
     ]
    }
   ],
   "source": [
    "aug_train_a = aug_group_by_a(train_df)\n",
    "print(len(aug_train_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27875b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extra_file.bucket_sampler import SortedSampler, BucketBatchSampler\n",
    "from extra_file.EMA import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e738c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "config = {\n",
    "        'train_file_path': 'data/train.json',\n",
    "        'dev_file_path': 'data/dev.json',\n",
    "        'test_file_path': 'data/test.json',\n",
    "        'output_path': '.',\n",
    "        'model_path': '../NLP_Project/dataset/BERT_model/',\n",
    "        'batch_size': 16,\n",
    "        'num_epochs': 1,\n",
    "        'max_seq_len': 64,\n",
    "        'learning_rate': 2e-5,\n",
    "        'weight_decay': 0.01,\n",
    "        'use_bucket': True,\n",
    "        'bucket_multiplier': 200,\n",
    "        'unsup_data_ratio': 1.5,\n",
    "        'uda_softmax_temp': 0.4,\n",
    "        'uda_confidence_threshold': 0.8,\n",
    "        'device': 'cuda',\n",
    "        'n_gpus': 0,\n",
    "        'logging_step': 400,\n",
    "        'ema_start_step': 500,\n",
    "        'ema_start': False,\n",
    "        'seed': 2021\n",
    "}\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    config['device'] = 'cpu'\n",
    "else:\n",
    "    config['n_gpus'] = torch.cuda.device_count()\n",
    "    config['batch_size'] *= config['n_gpus']\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d766b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(config['model_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "993467b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer):\n",
    "    inputs_dict = tokenizer.encode_plus(sentence_a, sentence_b, add_special_tokens=True,\n",
    "                          return_token_type_ids=True, return_attention_mask=True)\n",
    "    \n",
    "    inputs['input_ids'].append(inputs_dict['input_ids'])\n",
    "    inputs['token_type_ids'].append(inputs_dict['token_type_ids'])\n",
    "    inputs['attention_mask'].append(inputs_dict['attention_mask'])\n",
    "    inputs['labels'].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "076003d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对偶数据增强\n",
    "def build_unsup_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer):\n",
    "    lr_inputs_dict = tokenizer.encode_plus(sentence_a, sentence_b, add_special_tokens=True,\n",
    "                          return_token_type_ids=True, return_attention_mask=True)\n",
    "                     \n",
    "    rl_inputs_dict = tokenizer.encode_plus(sentence_b, sentence_a, add_special_tokens=True,\n",
    "                          return_token_type_ids=True, return_attention_mask=True)\n",
    "    \n",
    "    inputs['input_ids'].append((lr_inputs_dict['input_ids'], rl_inputs_dict['input_ids']))\n",
    "    inputs['token_type_ids'].append((lr_inputs_dict['token_type_ids'],rl_inputs_dict['token_type_ids']))\n",
    "    inputs['attention_mask'].append((lr_inputs_dict['attention_mask'],rl_inputs_dict['attention_mask']))\n",
    "    inputs['labels'].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4b81765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(config, tokenizer):\n",
    "    train_df = parse_data(config['train_file_path'], data_type='train')\n",
    "    dev_df = parse_data(config['dev_file_path'], data_type='dev')\n",
    "    test_df = parse_data(config['test_file_path'], data_type='test')\n",
    "    \n",
    "    data_df = {'train': train_df, 'dev': dev_df, 'test': test_df}\n",
    "    processed_data = {}\n",
    "    unsup_data = defaultdict(list)\n",
    "    \n",
    "    for data_type, df in data_df.items():\n",
    "        inputs = defaultdict(list)\n",
    "        if data_type == 'train':\n",
    "            reversed_inputs = defaultdict(list)\n",
    "        \n",
    "        for i, row in tqdm(df.iterrows(), desc=f'Processing {data_type} data', total=len(df)):\n",
    "            label = 0 if data_type == 'test' else row[2]\n",
    "            sentence_a, sentence_b = row[0], row[1]\n",
    "            build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer)\n",
    "            \n",
    "            if data_type.startswith(\"test\"):\n",
    "                build_bert_inputs(inputs, label, sentence_b, sentence_a, tokenizer)\n",
    "            \n",
    "            build_unsup_bert_inputs(unsup_data, label, sentence_a, sentence_b, tokenizer)\n",
    "        processed_data[data_type] = inputs\n",
    "        \n",
    "    processed_data['unsup_data'] = unsup_data\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b7f6e62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3327fb4f59e4f5f92c3decefd13eae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading train data:   0%|          | 0/34334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6acd6945afac4bacb7eef58ab2612188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dev data:   0%|          | 0/4316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a67c448a2dc4c2a9966d73691e76d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading test data:   0%|          | 0/3861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a002f076fc945c19568926895805482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train data:   0%|          | 0/34334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7030023e7b24fbc8c467fbf043a9d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dev data:   0%|          | 0/4316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686735713d734c96b2a9aa33f5054cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test data:   0%|          | 0/3861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = read_data(config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1572ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AFQMCDataset(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        super(AFQMCDataset, self).__init__()\n",
    "        self.data_dict = data_dict\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        data = (self.data_dict['input_ids'][idx], self.data_dict['token_type_ids'][idx],\n",
    "                self.data_dict['attention_mask'][idx], self.data_dict['labels'][idx])\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_dict['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "564c6a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    def __init__(self, max_seq_len, tokenizer):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def pad_and_truncate(self, input_ids_list, token_type_ids_list, attention_mask_list, labels_list, max_seq_len):\n",
    "        input_ids = torch.zeros((len(input_ids_list), max_seq_len), dtype=torch.long)\n",
    "        token_type_ids = torch.zeros_like(input_ids)\n",
    "        attention_mask = torch.zeros_like(input_ids)\n",
    "        \n",
    "        for i in range(len(input_ids_list)):\n",
    "            seq_len = len(input_ids_list[i])\n",
    "            if seq_len <= max_seq_len:\n",
    "                input_ids[i, :seq_len] = torch.tensor(input_ids_list[i], dtype=torch.long)\n",
    "                token_type_ids[i, :seq_len] = torch.tensor(token_type_ids_list[i], dtype=torch.long)\n",
    "                attention_mask[i, :seq_len] = torch.tensor(attention_mask_list[i], dtype=torch.long)\n",
    "            else:\n",
    "                input_ids[i] = torch.tensor(input_ids_list[i][:max_seq_len-1] + [self.tokenizer.sep_token_id], dtype=torch.long)\n",
    "                token_type_ids[i] = torch.tensor(token_type_ids_list[i][:max_seq_len], dtype=torch.long)\n",
    "                attention_mask[i] = torch.tensor(attention_mask_list[i][:max_seq_len], dtype=torch.long)\n",
    "        \n",
    "        labels = torch.tensor(labels_list, dtype=torch.long)\n",
    "        \n",
    "        return input_ids, token_type_ids, attention_mask, labels\n",
    "        \n",
    "    def __call__(self, examples):\n",
    "        input_ids_list, token_type_ids_list, attention_mask_list, labels_list = list(zip(*examples))\n",
    "        cur_max_seq_len = max(len(input_ids) for input_ids in input_ids_list)\n",
    "        max_seq_len = min(cur_max_seq_len, self.max_seq_len)\n",
    "        \n",
    "        input_ids, token_type_ids, attention_mask, labels = self.pad_and_truncate(input_ids_list, token_type_ids_list, attention_mask_list, labels_list, max_seq_len)\n",
    "        data_dict = {\n",
    "            'input_ids': input_ids,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e5bdea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = Collator(config['max_seq_len'], tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
