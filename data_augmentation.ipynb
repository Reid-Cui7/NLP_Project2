{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb7616c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /data2/cuimengjun/anaconda3/envs/d2l/lib/python3.8/site-packages/synonyms/data/vocab.txt ...\n",
      "Loading model from cache /tmp/jieba.u1ee567e3d900769013ee0717e970f060.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Synonyms: v3.18.0, Project home: https://github.com/chatopera/Synonyms/\n",
      "\n",
      " Project Sponsored by Chatopera\n",
      "\n",
      "  deliver your chatbots with Chatopera Cloud Services --> https://bot.chatopera.com\n",
      "\n",
      ">> Synonyms load wordseg dict [/data2/cuimengjun/anaconda3/envs/d2l/lib/python3.8/site-packages/synonyms/data/vocab.txt] ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.469 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Synonyms on loading stopwords [/data2/cuimengjun/anaconda3/envs/d2l/lib/python3.8/site-packages/synonyms/data/stopwords.txt] ...\n",
      ">> Synonyms on loading vectors [/data2/cuimengjun/anaconda3/envs/d2l/lib/python3.8/site-packages/synonyms/data/words.vector.gz] ...\n"
     ]
    }
   ],
   "source": [
    "import synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112af3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['爸爸', '妈妈', '爷爷', '奶奶', '老婆', '姐姐', '爸妈', '老公', '老爸', '外婆'],\n",
       " [1.0,\n",
       "  0.9197904,\n",
       "  0.8369896,\n",
       "  0.83030784,\n",
       "  0.80404544,\n",
       "  0.79813045,\n",
       "  0.7979228,\n",
       "  0.7714372,\n",
       "  0.77116084,\n",
       "  0.7584483])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms.nearby(\"爸爸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "044911b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {word.strip() for word in open('data/baidu_stopwords.txt', 'r', encoding='utf-8').readlines()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf6658be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同义词替换(SR): 从句子中随机选择n个不是停用词的词 用随机选择的同义词之一替换这些单词中的每一个\n",
    "import random\n",
    "\n",
    "def get_synonyms(word):\n",
    "    sys = set(synonyms.nearby(word)[0])\n",
    "    if word in sys:\n",
    "        sys.remove(word)\n",
    "        \n",
    "    return list(sys)\n",
    "\n",
    "def synonym_replacement(words, n):\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "    random.shuffle(random_word_list)\n",
    "    \n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced > n:\n",
    "            break\n",
    "            \n",
    "    sentence = ' '.join(new_words)\n",
    "    new_words = sentence.split(' ')\n",
    "    \n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f2e7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机删除(RD): 以概率p随机删除句子中的每个单词\n",
    "\n",
    "def random_deletion(words, p):\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > p:\n",
    "            new_words.append(word)\n",
    "            \n",
    "    if len(new_words) == 0:\n",
    "        random_int = random.randint(0, len(words) - 1)\n",
    "        return [words[random_int]]\n",
    "    \n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c19abe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机交换(RS): 随机选择句子中的两个单词并交换他们的位置 做n次\n",
    "def swap_word(new_words):\n",
    "    random_idx_1 = random.randint(0, len(new_words) - 1)\n",
    "    random_idx_2 = random_idx_1\n",
    "    \n",
    "    cnt = 0\n",
    "    while random_idx_2 == random_idx_1:\n",
    "        random_idx_2 = random.randint(0, len(new_words) - 1)\n",
    "        cnt += 1\n",
    "        if cnt > 3:\n",
    "            return new_words\n",
    "        \n",
    "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
    "    \n",
    "    return new_words\n",
    "\n",
    "def random_swap(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        new_words = swap_word(new_words)\n",
    "    \n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd9e57be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随即插入(RI): 在句子中随机找到一个词, 并找出其同义词, 且该词不是停用词, 将该同义词插入句子的随机位置 做n次\n",
    "def add_word(new_words):\n",
    "    synonyms = []\n",
    "    cnt = 0\n",
    "    while len(synonyms) < 1:\n",
    "        random_word = new_words[random.randint(0, len(new_words) - 1)]\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        cnt += 1\n",
    "        if cnt >= 10:\n",
    "            return\n",
    "    \n",
    "def random_insert(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        add_word(new_words)\n",
    "        \n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e218fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
    "    words = synonyms.seg(sentence)[0]\n",
    "    num_words = len(words)\n",
    "    augmented_sentence = []\n",
    "    num_new_per_technique = int(num_aug / 4) + 1\n",
    "    \n",
    "    n_sr = max(1, (int)(alpha_sr * num_words))\n",
    "    n_ri = max(1, int(alpha_ri*num_words))\n",
    "    n_rs = max(1, int(alpha_rs*num_words))\n",
    "    \n",
    "    # sr\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = synonym_replacement(words, n_sr)\n",
    "        augmented_sentence.append(' '.join(a_words))\n",
    "    # ri\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_insert(words, n_ri)\n",
    "        augmented_sentence.append(' '.join(a_words))\n",
    "    # rs\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_swap(words, n_rs)\n",
    "        augmented_sentence.append(' '.join(a_words))\n",
    "    # rd\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_deletion(words, p_rd)\n",
    "        augmented_sentence.append(' '. join(a_words))\n",
    "        \n",
    "    random.shuffle(augmented_sentence)\n",
    "    if num_aug >= 1:\n",
    "        augmented_sentence = augmented_sentence[:num_aug]\n",
    "    else:\n",
    "        keep_prob = num_aug / len(augmented_sentence)\n",
    "        augmented_sentence = [s for s in augmented_sentence if random.uniform(0, 1) < keep_prob]\n",
    "    \n",
    "    return augmented_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeee8198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['目前', '华为', '部分', '型号', '的', '手机', '产品', '出现', '货', '少', '的', '现象'], ['t', 'nr', 'n', 'n', 'uj', 'n', 'n', 'v', 'n', 'a', 'uj', 'n'])\n"
     ]
    }
   ],
   "source": [
    "words = synonyms.seg('目前华为部分型号的手机产品出现货少的现象')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e06d4191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15 日 以来 ， 台积电 、 高通 、 三星 华为 的 重要 合作伙伴 ， 没有 美国 的 相关 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货 少 的 现象 ， 若 该 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
       " '9 月 三星 日 以来 ， 台积电 、 高通 、 15 等 华为 的 重要 合作伙伴 ， ， 没有 给 的 相关 许可证 ， 都 无法 供应 芯片 美国 华为 企业 而 中芯国际 等 国产 芯片 只要 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货 业务 的 该 ， 若 现象 形势 华为 下去 ， 持续 手机 少 将 遭受 重创 。',\n",
       " '9 月 15 日 以来 ， Tektronix 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 全都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 化工企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 机型 的 笔记型电脑 产品 出现 货品 贵 的 现象 ， 若 该 形势 持续 下去 ， 华为 笔记型电脑 业务 将 遭受 受创 。',\n",
       " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 SAP ， 只要 没有 英国 的 相关 许可证 ， 即便 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 导入 英国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货 多一点 的 弊病 ， 若 该 宏观经济形势 持续 下去 ， 华为 手机 业务 将 遭受 重挫 。',\n",
       " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货 少 的 现象 ， 若 该 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
       " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 ， 的 重要 合作伙伴 ， 形势 没有 美国 无法 相关 许可证 ， 都 的 供应 芯片 给 华为 ， 只要 中芯国际 华为 国产 芯片 企业 ， 也 因 部分 美国 技术 ， 而 无法 供货 给 华为 。 手机 华为 采用 型号 的 手机 产品 出现 货 少 的 现象 ， 若 该 而 持续 下去 华为 等 目前 业务 将 遭受 重创 。',\n",
       " '华为 月 15 供货 以来 ， 台积电 、 高通 、 三星 等 9 的 重要 合作伙伴 目前 只要 没有 美国 的 相关 许可证 ， 都 无法 供应 芯片 给 华为 华为 而 中芯国际 等 国产 ， 企业 ， 也 因 采用 美国 技术 芯片 部分 无法 日 给 华为 。 ， 华为 而 型号 的 手机 产品 出现 货 少 的 现象 ， 若 该 形势 持续 下去 ， ， 手机 业务 将 遭受 重创 。',\n",
       " '9 月 15 日 以来 ， Tektronix 、 高通 、 三星 等 三星 的 重要 合作伙伴 ， 只要 没有 古巴 的 相关 许可证 ， 都 无法 供应 芯片 给 三星 ， 而 Nashik 等 投放市场 芯片 企业 ， 也 因 采用 古巴 核心技术 ， 而 无法 供货 给 三星 。 目前 三星 部分 型号 的 手机 产品 出现 货 少 的 异常现象 ， 若 该 形势 持续 下去 ， 三星 手机 业务 将 遭受 重击 。',\n",
       " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 重要 合作伙伴 ， 只要 没有 美国 的 相关 ， 都 无法 供应 芯片 给 华为 而 中芯国际 等 国产 芯片 企业 因 采用 美国 技术 ， 而 无法 供货 给 。 目前 华为 部分 型号 的 手机 产品 出现 货 少 的 现象 ， 若 该 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda('9月15日以来，台积电、高通、三星等华为的重要合作伙伴，只要没有美国的相关许可证，都无法供应芯片给华为，而中芯国际等国产芯片企业，也因采用美国技术，而无法供货给华为。目前华为部分型号的手机产品出现货少的现象，若该形势持续下去，华为手机业务将遭受重创。')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e96edb",
   "metadata": {},
   "source": [
    "#### 闭包数据增强\n",
    "数据集中每条数据有两个句子\\\n",
    "a和b相似 a和c相似\\\n",
    "则b和c相似\\\n",
    "a和d不相似 0\\\n",
    "则b和d不相似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64d3bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "def parse_data(path, data_type='train'):\n",
    "    sentence_a, sentence_b, labels = [], [], []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f.readlines(), desc=f'Reading {data_type} data'):\n",
    "            line = json.loads(line)\n",
    "            sentence_a.append(line['sentence1'])\n",
    "            sentence_b.append(line['sentence2'])\n",
    "            if data_type != 'test':\n",
    "                labels.append(int(line['label']))\n",
    "            else:\n",
    "                labels.append(0)\n",
    "    df = pd.DataFrame(zip(sentence_a, sentence_b, labels), columns=['text_a', 'text_b', 'labels'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6af9d0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f727ba7abde0456f961cb35bba511d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading train data:   0%|          | 0/34334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = parse_data('data/train.json', data_type='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8b5cb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>蚂蚁借呗等额还款可以换成先息后本吗</td>\n",
       "      <td>借呗有先息到期还本吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>蚂蚁花呗说我违约一次</td>\n",
       "      <td>蚂蚁花呗违约行为是什么</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>帮我看一下本月花呗账单有没有结清</td>\n",
       "      <td>下月花呗账单</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              text_a       text_b  labels\n",
       "0  蚂蚁借呗等额还款可以换成先息后本吗   借呗有先息到期还本吗       0\n",
       "1         蚂蚁花呗说我违约一次  蚂蚁花呗违约行为是什么       0\n",
       "2   帮我看一下本月花呗账单有没有结清       下月花呗账单       0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ee7f404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为何我的花呗不能用\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "          text_a       text_b  labels\n",
      "13425  为何我的花呗不能用     为什么花呗用不了       1\n",
      "14005  为何我的花呗不能用    花呗不能正常使用了       1\n",
      "15175  为何我的花呗不能用  我现在是不是用不了花呗       0\n"
     ]
    }
   ],
   "source": [
    "for data in train_df.groupby(by=['text_a']):\n",
    "    if len(data[1]) == 3:\n",
    "        print(data[0])\n",
    "        print(type(data[1]))\n",
    "        print(data[1])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "040e1495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def aug_group_by_a(df):\n",
    "    aug_data = defaultdict(list)\n",
    "    # 以text_a句子为a\n",
    "    for g, data in df.groupby(by=['text_a']):\n",
    "        if len(data) < 2:\n",
    "            continue\n",
    "        for i in range(len(data)):\n",
    "            for j in range(i + 1, len(data)):\n",
    "                # 取出b的值 a, b的label\n",
    "                row_i_text = data.iloc[i, 1]\n",
    "                row_i_label = data.iloc[i, 2]\n",
    "                # 取出c的值 a, c的label\n",
    "                row_j_text = data.iloc[j, 1]\n",
    "                row_j_label = data.iloc[j, 2]\n",
    "                \n",
    "                if row_i_label == row_j_label == 0:\n",
    "                    continue\n",
    "                aug_label = 1 if row_i_label == row_j_label == 1 else 0\n",
    "                \n",
    "                aug_data['text_a'].append(row_i_text)\n",
    "                aug_data['text_b'].append(row_j_text)\n",
    "                aug_data['labels'].append(aug_label)\n",
    "    \n",
    "    return pd.DataFrame(aug_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6555e7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228\n"
     ]
    }
   ],
   "source": [
    "aug_train_a = aug_group_by_a(train_df)\n",
    "print(len(aug_train_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27875b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extra_file.bucket_sampler import SortedSampler, BucketBatchSampler\n",
    "from extra_file.EMA import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e738c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "config = {\n",
    "        'train_file_path': 'data/train.json',\n",
    "        'dev_file_path': 'data/dev.json',\n",
    "        'test_file_path': 'data/test.json',\n",
    "        'output_path': '.',\n",
    "        'model_path': '../NLP_Project/dataset/BERT_model/',\n",
    "        'batch_size': 16,\n",
    "        'num_epochs': 2,\n",
    "        'max_seq_len': 64,\n",
    "        'learning_rate': 2e-5,\n",
    "        'weight_decay': 0.01,\n",
    "        'use_bucket': True,\n",
    "        'bucket_multiplier': 200,\n",
    "        'unsup_data_ratio': 1.5,\n",
    "        'uda_softmax_temp': 0.4,\n",
    "        'uda_confidence_threshold': 0.8,\n",
    "        'device': 'cuda',\n",
    "        'n_gpus': 0,\n",
    "        'logging_step': 400,\n",
    "        'ema_start_step': 500,\n",
    "        'ema_start': False,\n",
    "        'seed': 2021\n",
    "}\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    config['device'] = 'cpu'\n",
    "else:\n",
    "    config['n_gpus'] = torch.cuda.device_count()\n",
    "    config['batch_size'] *= config['n_gpus']\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d766b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(config['model_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "993467b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer):\n",
    "    inputs_dict = tokenizer.encode_plus(sentence_a, sentence_b, add_special_tokens=True,\n",
    "                          return_token_type_ids=True, return_attention_mask=True)\n",
    "    \n",
    "    inputs['input_ids'].append(inputs_dict['input_ids'])\n",
    "    inputs['token_type_ids'].append(inputs_dict['token_type_ids'])\n",
    "    inputs['attention_mask'].append(inputs_dict['attention_mask'])\n",
    "    inputs['labels'].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "076003d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对偶数据增强\n",
    "def build_unsup_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer):\n",
    "    lr_inputs_dict = tokenizer.encode_plus(sentence_a, sentence_b, add_special_tokens=True,\n",
    "                          return_token_type_ids=True, return_attention_mask=True)\n",
    "                     \n",
    "    rl_inputs_dict = tokenizer.encode_plus(sentence_b, sentence_a, add_special_tokens=True,\n",
    "                          return_token_type_ids=True, return_attention_mask=True)\n",
    "    \n",
    "    inputs['input_ids'].append((lr_inputs_dict['input_ids'], rl_inputs_dict['input_ids']))\n",
    "    inputs['token_type_ids'].append((lr_inputs_dict['token_type_ids'],rl_inputs_dict['token_type_ids']))\n",
    "    inputs['attention_mask'].append((lr_inputs_dict['attention_mask'],rl_inputs_dict['attention_mask']))\n",
    "    inputs['labels'].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4b81765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(config, tokenizer):\n",
    "    train_df = parse_data(config['train_file_path'], data_type='train')\n",
    "    dev_df = parse_data(config['dev_file_path'], data_type='dev')\n",
    "    test_df = parse_data(config['test_file_path'], data_type='test')\n",
    "    \n",
    "    data_df = {'train': train_df, 'dev': dev_df, 'test': test_df}\n",
    "    processed_data = {}\n",
    "    unsup_data = defaultdict(list)\n",
    "    \n",
    "    for data_type, df in data_df.items():\n",
    "        inputs = defaultdict(list)\n",
    "        if data_type == 'train':\n",
    "            reversed_inputs = defaultdict(list)\n",
    "        \n",
    "        for i, row in tqdm(df.iterrows(), desc=f'Processing {data_type} data', total=len(df)):\n",
    "            label = 0 if data_type == 'test' else row[2]\n",
    "            sentence_a, sentence_b = row[0], row[1]\n",
    "            build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer)\n",
    "            \n",
    "            if data_type.startswith(\"test\"):\n",
    "                build_bert_inputs(inputs, label, sentence_b, sentence_a, tokenizer)\n",
    "            \n",
    "            build_unsup_bert_inputs(unsup_data, label, sentence_a, sentence_b, tokenizer)\n",
    "        processed_data[data_type] = inputs\n",
    "        \n",
    "    processed_data['unsup_data'] = unsup_data\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7f6e62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f6b60b8dff4ee997974f1e6ba73248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading train data:   0%|          | 0/34334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f438feee56241f2a22a11d72cb306c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dev data:   0%|          | 0/4316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafc420114c143429fae8e036688def7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading test data:   0%|          | 0/3861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e21248c75ea43f4a4d5dcd1513a0d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train data:   0%|          | 0/34334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9633bc373e0e415c970f67c48a49e21f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dev data:   0%|          | 0/4316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4047677621c741a29026b5faeb2d0e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test data:   0%|          | 0/3861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = read_data(config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1572ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AFQMCDataset(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        super(AFQMCDataset, self).__init__()\n",
    "        self.data_dict = data_dict\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        data = (self.data_dict['input_ids'][idx], self.data_dict['token_type_ids'][idx],\n",
    "                self.data_dict['attention_mask'][idx], self.data_dict['labels'][idx])\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_dict['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "564c6a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    def __init__(self, max_seq_len, tokenizer):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def pad_and_truncate(self, input_ids_list, token_type_ids_list, attention_mask_list, labels_list, max_seq_len):\n",
    "        input_ids = torch.zeros((len(input_ids_list), max_seq_len), dtype=torch.long)\n",
    "        token_type_ids = torch.zeros_like(input_ids)\n",
    "        attention_mask = torch.zeros_like(input_ids)\n",
    "        \n",
    "        for i in range(len(input_ids_list)):\n",
    "            seq_len = len(input_ids_list[i])\n",
    "            if seq_len <= max_seq_len:\n",
    "                input_ids[i, :seq_len] = torch.tensor(input_ids_list[i], dtype=torch.long)\n",
    "                token_type_ids[i, :seq_len] = torch.tensor(token_type_ids_list[i], dtype=torch.long)\n",
    "                attention_mask[i, :seq_len] = torch.tensor(attention_mask_list[i], dtype=torch.long)\n",
    "            else:\n",
    "                input_ids[i] = torch.tensor(input_ids_list[i][:max_seq_len-1] + [self.tokenizer.sep_token_id], dtype=torch.long)\n",
    "                token_type_ids[i] = torch.tensor(token_type_ids_list[i][:max_seq_len], dtype=torch.long)\n",
    "                attention_mask[i] = torch.tensor(attention_mask_list[i][:max_seq_len], dtype=torch.long)\n",
    "        \n",
    "        labels = torch.tensor(labels_list, dtype=torch.long)\n",
    "        \n",
    "        return input_ids, token_type_ids, attention_mask, labels\n",
    "        \n",
    "    def __call__(self, examples):\n",
    "        input_ids_list, token_type_ids_list, attention_mask_list, labels_list = list(zip(*examples))\n",
    "        cur_max_seq_len = max(len(input_ids) for input_ids in input_ids_list)\n",
    "        max_seq_len = min(cur_max_seq_len, self.max_seq_len)\n",
    "        \n",
    "        input_ids, token_type_ids, attention_mask, labels = self.pad_and_truncate(input_ids_list, token_type_ids_list, attention_mask_list, labels_list, max_seq_len)\n",
    "        data_dict = {\n",
    "            'input_ids': input_ids,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5bdea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = Collator(config['max_seq_len'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30426292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDA\n",
    "class UnsupAFQMCDataset(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        super(UnsupAFQMCDataset, self).__init__()\n",
    "        self.data_dict = data_dict\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.data_dict['input_ids'][idx]\n",
    "        token_type_ids = self.data_dict['token_type_ids'][idx]\n",
    "        attention_mask = self.data_dict['attention_mask'][idx]\n",
    "        labels = self.data_dict['labels'][idx]\n",
    "        \n",
    "        return (input_ids[0], token_type_ids[0], attention_mask[0],\n",
    "                 input_ids[1], token_type_ids[1], attention_mask[1],\n",
    "                 labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_dict['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b52bc0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnsupCollator(Collator):\n",
    "    def __init__(self, max_seq_len, tokenizer):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, examples):\n",
    "        (ab_input_ids_list, ab_token_type_ids_list, ab_attention_mask_list,\n",
    "         ba_input_ids_list, ba_token_type_ids_list, ba_attention_mask_list, labels_list) = list(zip(*examples))\n",
    "        \n",
    "        cur_max_seq_len = max(len(input_ids) for input_ids in ab_input_ids_list)\n",
    "        max_seq_len = min(cur_max_seq_len, self.max_seq_len)\n",
    "        \n",
    "        # 分批整ab, ba\n",
    "        ab_input_ids, ab_token_type_ids, ab_attention_mask, labels = self.pad_and_truncate(ab_input_ids_list, ab_token_type_ids_list, ab_attention_mask_list, labels_list, max_seq_len)\n",
    "        ba_input_ids, ba_token_type_ids, ba_attention_mask, labels = self.pad_and_truncate(ba_input_ids_list, ba_token_type_ids_list, ba_attention_mask_list, labels_list, max_seq_len)\n",
    "\n",
    "        data_dict = {\n",
    "            'ab_input_ids': ab_input_ids,\n",
    "            'ab_token_type_ids': ab_token_type_ids,\n",
    "            'ab_attention_mask': ab_attention_mask,\n",
    "            'ba_input_ids': ba_input_ids,\n",
    "            'ba_token_type_ids': ba_token_type_ids,\n",
    "            'ba_attention_mask': ba_attention_mask,\n",
    "            'labels': labels\n",
    "\n",
    "        }\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c732020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from extra_file.bucket_sampler import BucketBatchSampler\n",
    "\n",
    "def build_dataloader(config, data, tokenizer):\n",
    "    train_dataset = AFQMCDataset(data['train'])\n",
    "    dev_dataset = AFQMCDataset(data['dev'])\n",
    "    test_dataset = AFQMCDataset(data['test'])\n",
    "    unsup_dataset = UnsupAFQMCDataset(data['unsup_data'])\n",
    "    \n",
    "    collate_fn = Collator(config['max_seq_len'], tokenizer)\n",
    "    unsup_collate_fn = UnsupCollator(config['max_seq_len'], tokenizer)\n",
    "    \n",
    "    if config['use_bucket']:\n",
    "        # 监督数据\n",
    "        train_sampler = RandomSampler(train_dataset)\n",
    "        bucket_sampler = BucketBatchSampler(train_sampler, batch_size=config['batch_size'],\n",
    "                                            drop_last=False, sort_key=lambda x: len(train_dataset[x][0]),\n",
    "                                            bucket_size_multiplier=config['bucket_multiplier'])\n",
    "        train_dataloader = DataLoader(dataset=train_dataset, batch_sampler=bucket_sampler, num_workers=4, collate_fn=collate_fn)\n",
    "        \n",
    "        # 无监督数据\n",
    "        unsup_sampler = RandomSampler(unsup_dataset)\n",
    "        unsup_bucket_sampler = BucketBatchSampler(unsup_sampler, batch_size=int(config['batch_size'] * config['unsup_data_ratio']),\n",
    "                                                  drop_last=False, sort_key=lambda x: len(unsup_dataset[x][0]),\n",
    "                                                  bucket_size_multiplier=config['bucket_multiplier'])\n",
    "        unsup_dataloader = DataLoader(dataset=unsup_dataset, batch_sampler=unsup_bucket_sampler, num_workers=4, collate_fn=unsup_collate_fn)\n",
    "    \n",
    "    else:\n",
    "        # 监督数据\n",
    "        train_dataloader = DataLoader(dataset=train_dataset, batch_size=config['batch_size'],\n",
    "                                     shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "        # 无监督数据\n",
    "        unsup_dataloader = DataLoader(dataset=unsup_dataset, batch_size=int(config['batch_size']*config['unsup_data_ratio']),\n",
    "                                      shuffle=True, num_workers=4, collate_fn=unsup_collate_fn)\n",
    "    \n",
    "    dev_dataloader = DataLoader(dataset=dev_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "    return unsup_dataloader, train_dataloader, dev_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43ee009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsup_dataloader, train_dataloader, dev_dataloader, test_dataloader = build_dataloader(config, data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7de226ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2769, 7566,  ..., 1446,  102,    0],\n",
      "        [ 101, 6010, 6009,  ..., 3299,  102,    0],\n",
      "        [ 101, 2769, 4638,  ..., 4638,  102,    0],\n",
      "        ...,\n",
      "        [ 101, 2769, 1762,  ..., 1446, 1568,  102],\n",
      "        [ 101, 5709, 1446,  ..., 6589, 1408,  102],\n",
      "        [ 101, 2769, 4638,  ..., 1184, 6820,  102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])}\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataloader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1549db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def evaluation(config, model, val_dataloader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    val_loss = 0.\n",
    "    val_iterator = tqdm(val_dataloader, desc='Evaluation', total=len(val_dataloader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_iterator:\n",
    "            labels.append(batch['labels'])\n",
    "            batch_cuda = {item: value.to(config['device']) for item, value in list(batch.items())}\n",
    "            batch_cuda['mode'] = 'val'\n",
    "            loss, logits = model(**batch_cuda)[:2]\n",
    "\n",
    "            if config['n_gpus'] > 1:\n",
    "                loss = loss.mean()\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds.append(logits.argmax(dim=-1).detach().cpu())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    labels = torch.cat(labels, dim=0).numpy()\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    f1 = metrics.f1_score(labels, preds)\n",
    "    acc = metrics.accuracy_score(labels, preds)\n",
    "    return avg_val_loss, f1, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d692a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "class BertForAFQMC(BertForSequenceClassification):\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                token_type_ids,\n",
    "                attention_mask,\n",
    "                labels=None,\n",
    "                mode='train'):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        outputs = (logits, )\n",
    "\n",
    "        if mode == 'val':\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "            loss = loss_fct(logits, labels.view(-1))\n",
    "                \n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "255cc33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return grad_data: 需要计算梯度 需要进行反向传播的数据\n",
    "# Return no_grad_data: 不需要计算梯度 不需要进行反向传播的数据\n",
    "def get_data(sup_batch, unsup_batch, config):\n",
    "    grad_data, no_grad_data = {}, {}\n",
    "    # sup_batch [bs, seq_len]  unsup_batch [bs, seq_len]\n",
    "    sup_max_len = sup_batch['input_ids'].size(1)\n",
    "    unsup_max_len = unsup_batch['ba_input_ids'].size(1)\n",
    "    cur_max_len = max(sup_max_len, unsup_max_len)\n",
    "    \n",
    "    for item, sup_value in sup_batch.items():\n",
    "        if item == 'labels':\n",
    "            grad_data[item] = sup_value.to(config['device'])\n",
    "            continue\n",
    "        \n",
    "        ba_unsup_value, ab_unsup_value = unsup_batch[f'ba_{item}'], unsup_batch[f'ab_{item}']\n",
    "        # 谁短补水\n",
    "        if sup_max_len == cur_max_len:\n",
    "            padding_value = torch.zeros((ba_unsup_value.size(0), cur_max_len - unsup_max_len),\n",
    "                                        dtype=ba_unsup_value.dtype)\n",
    "            ba_unsup_value = torch.cat([ba_unsup_value, padding_value], dim=-1)\n",
    "        else:\n",
    "            padding_value = torch.zeros((sup_value.size(0), cur_max_len - sup_max_len),\n",
    "                                        dtype=sup_value.dtype)\n",
    "            sup_value = torch.cat([sup_value, padding_value], dim=-1)\n",
    "        # sup_batch ba cat\n",
    "        grad_value = torch.cat([sup_value, ba_unsup_value], dim=0)\n",
    "        grad_data[item] = grad_value.to(config['device'])\n",
    "        no_grad_data[item] = ab_unsup_value.to(config['device'])\n",
    "        \n",
    "    return grad_data, no_grad_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76e41b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3431,  0.2891,  0.0157],\n",
      "        [-0.8506, -0.2726,  0.2910]])\n",
      "tensor([[0.3748, 0.3551, 0.2702],\n",
      "        [0.1691, 0.3014, 0.5295]])\n",
      "tensor([[0.4320, 0.3774, 0.1906],\n",
      "        [0.0442, 0.1877, 0.7680]])\n"
     ]
    }
   ],
   "source": [
    "logits = torch.randn(2,3)\n",
    "print(logits)\n",
    "t_softmax = torch.softmax(logits, dim=1)\n",
    "print(t_softmax)\n",
    "t_sharpen = torch.softmax(logits/0.4, dim=1)\n",
    "print(t_sharpen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b89ebfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 无监督数据 ab 只需要正向传播\n",
    "def forward_no_grad(no_grad_data, config, model):\n",
    "    with torch.no_grad():\n",
    "        no_grad_logits = model(**no_grad_data)[0]\n",
    "        # sharpen\n",
    "        no_grad_probs = torch.softmax(no_grad_logits / config['uda_softmax_temp'], dim=-1)\n",
    "        largest_probs, _ = no_grad_probs.max(dim=-1)\n",
    "        unsup_loss_mask = largest_probs.gt(config['uda_confidence_threshold']).float()\n",
    "        # unsup_loss_mask tensor9[True, False, True, False]\n",
    "    return unsup_loss_mask, no_grad_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8dc6d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tsa_threshold(total_steps, global_steps):\n",
    "    return np.exp((global_steps / total_steps - 1) * 5) / 2 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1f126c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_with_grad(unsup_loss_mask, unsup_probs, config, cur_bs,\n",
    "                      model, grad_data, total_steps, global_steps):\n",
    "    # 得到eta 随着训练进行阈值逐渐变大最后达到1 将所有的监督数据都使用了\n",
    "    tsa_threshold = get_tsa_threshold(total_steps, global_steps)\n",
    "    logits = model(**grad_data)[0]\n",
    "    \n",
    "    # 有监督损失部分\n",
    "    # cur_bs 无监督ba的batch_size\n",
    "    # 前面是train的sup_data 后面是unsup_data\n",
    "    sup_logits, unsup_logits = logits.split([logits.size(0) - cur_bs, cur_bs])\n",
    "    sup_labels = grad_data['labels'][:logits.size(0) - cur_bs]\n",
    "    per_example_loss = nn.CrossEntropyLoss(reduction='none')(sup_logits, sup_labels)\n",
    "    # 拿出正确标签对应的概率\n",
    "    correct_label_probs = torch.softmax(sup_logits, dim=-1).gather(dim=-1, index=sup_labels.view(-1, 1))\n",
    "    # 监督数据中过于自信的不要留下小于tsa_threshold的计算损失\n",
    "    sup_loss_mask = correct_label_probs.le(tsa_threshold).squeeze().float()\n",
    "    # 应用mask掩盖有监督数据中过于自信的样本\n",
    "    per_example_loss *= sup_loss_mask\n",
    "    sup_loss = per_example_loss.sum() / max(sup_loss_mask.sum(), 1) # 有效个数\n",
    "    \n",
    "    # 无监督损失部分\n",
    "    unsup_log_probs = torch.log_softmax(unsup_logits, dim=-1)\n",
    "    per_example_kl_loss = nn.KLDivLoss(reduction='none')(unsup_log_probs, unsup_probs).sum(dim=-1)\n",
    "    # 应用mask掩盖无监督数据中不自信的样本\n",
    "    per_example_kl_loss *= unsup_loss_mask\n",
    "    unsup_loss = per_example_kl_loss.sum() / max(unsup_loss_mask.sum(), 1)\n",
    "    \n",
    "    # 可以后续加权两种损失\n",
    "    loss = unsup_loss + sup_loss\n",
    "    \n",
    "    if config['n_gpus'] > 1:\n",
    "        loss = loss.mean()\n",
    "        sup_loss = sup_loss.mean()\n",
    "        unsup_loss = unsup_loss.mean()\n",
    "        \n",
    "    return loss, tsa_threshold, unsup_loss, sup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61e78ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import trange\n",
    "import os\n",
    "\n",
    "def train(config, train_dataloader, dev_data_loader, unsup_dataloader=None):\n",
    "    model = BertForAFQMC.from_pretrained(config['model_path'])\n",
    "    optimizer = AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    model.to(config['device'])\n",
    "    # unsup_dataloader: train + dev + test 最大\n",
    "    total_steps = len(unsup_dataloader) * config['num_epochs']\n",
    "    epoch_iterator = trange(config['num_epochs'])\n",
    "    global_steps = 0\n",
    "    train_loss = 0.\n",
    "    logging_loss = 0.\n",
    "    best_acc = 0.\n",
    "    best_model_path = ''\n",
    "    \n",
    "    if config['n_gpus'] > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        \n",
    "    train_iterator = iter(train_dataloader)\n",
    "    for _ in epoch_iterator:\n",
    "        unsup_iterator = tqdm(unsup_dataloader, desc='Training', total=len(unsup_dataloader))\n",
    "        model.train()\n",
    "        for unsup_batch in unsup_iterator:\n",
    "            cur_bs = unsup_batch['ab_input_ids'].size(0)\n",
    "            try:\n",
    "                sup_batch = next(train_iterator)\n",
    "            except StopIteration:\n",
    "                train_iterator = iter(train_dataloader)\n",
    "                sup_batch = next(train_iterator)\n",
    "            \n",
    "            grad_data, no_grad_data = get_data(sup_batch, unsup_batch, config)\n",
    "            \n",
    "            unsup_loss_mask, unsup_probs = forward_no_grad(no_grad_data, config, model)\n",
    "            loss, tsa_threshold, unsup_loss, sup_loss = forward_with_grad(\n",
    "                unsup_loss_mask, unsup_probs, config, cur_bs, model, grad_data, total_steps, global_steps)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if config['ema_start']:\n",
    "                ema.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            global_steps += 1\n",
    "            \n",
    "            unsup_iterator.set_postfix_str(f'running training loss: {loss.item():.4f}')\n",
    "            if global_steps % config['logging_step'] == 0:\n",
    "                if global_steps >= config['ema_start_step'] and not config['ema_start']:\n",
    "                    print('\\n>>> EMA starting ...')\n",
    "                    config['ema_start'] = True\n",
    "                    ema = EMA(model.module if hasattr(model, 'module') else model, decay=0.999)\n",
    "\n",
    "                print_train_loss = (train_loss - logging_loss) / config['logging_step']\n",
    "                logging_loss = train_loss\n",
    "\n",
    "                if config['ema_start']:\n",
    "                    ema.apply_shadow()\n",
    "                val_loss, f1, acc = evaluation(config, model, dev_dataloader)\n",
    "\n",
    "                print_log = f'\\n>>> training loss: {print_train_loss:.6f}, valid loss: {val_loss:.6f}, '\n",
    "\n",
    "                if acc > best_acc:\n",
    "                    model_save_path = os.path.join(config['output_path'],\n",
    "                                                   f'checkpoint-{global_steps}-{acc:.6f}')\n",
    "                    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "                    model_to_save.save_pretrained(model_save_path)\n",
    "                    best_acc = acc\n",
    "                    best_model_path = model_save_path\n",
    "\n",
    "                print_log += f'valid f1: {f1:.6f}, valid acc: {acc:.6f}'\n",
    "\n",
    "                print(print_log)\n",
    "                model.train()\n",
    "                if config['ema_start']:\n",
    "                    ema.restore()\n",
    "\n",
    "    return model, best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d50ebef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../NLP_Project/dataset/BERT_model/ were not used when initializing BertForAFQMC: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForAFQMC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForAFQMC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForAFQMC were not initialized from the model checkpoint at ../NLP_Project/dataset/BERT_model/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6d779773784243b6c9b3faa23506b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dde7236f4d0450d87e4c964972e8cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/443 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c08fb148dff407b8ff9881ea5d3eb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> training loss: 1.041689, valid loss: 0.596706, valid f1: 0.473610, valid acc: 0.653383\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c97c6be18647a38fe99bfe09959319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/443 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> EMA starting ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f8924f206346068b3160ad7d146640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> training loss: 0.992483, valid loss: 0.530302, valid f1: 0.478155, valid acc: 0.701112\n"
     ]
    }
   ],
   "source": [
    "model, best_model_path = train(config, train_dataloader, dev_dataloader, unsup_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
